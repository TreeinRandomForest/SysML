{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92878a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def gen_data(N=100, d=10, low=0, high=10, target_idx=3):\n",
    "    data = np.random.randint(low=low, high=high, size=(N,d))\n",
    "    return data, data[:, target_idx]\n",
    "\n",
    "N = 5000\n",
    "low = 0 \n",
    "high = 10\n",
    "train_data, train_target = gen_data(N=N, low=low, high=high)\n",
    "test_data, test_target = gen_data(N=N, low=low, high=high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880aeee",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b79a9",
   "metadata": {},
   "source": [
    "An RNN will process a sequence of tokens. The pseudocode is something like the following:\n",
    "\n",
    "token_list = [...]\n",
    "\n",
    "hidden_vec = [0, ..., 0] #some fixed length or dimensionality\n",
    "\n",
    "for token in token_list:\n",
    "\n",
    "#lookup vector for each token from a hash table\n",
    "    token_vec = embedding_table[token]\n",
    "    \n",
    "    #use previous hidden_vec and current token_vec to update hidden_vec\n",
    "    #this is updating the state (hidden_vec) of the net using the new token\n",
    "    \n",
    "    hidden_vec = update(hidden_vec/previous state, token_vec/new data)\n",
    "    \n",
    "after the loop, hidden_vec encodes all the information about the input sequence and can be used to make a prediction\n",
    "\n",
    "prediction = pred(hidden_vec)\n",
    "\n",
    "for us, this could also be\n",
    "\n",
    "prediction = pred(hidden_vec, index)\n",
    "\n",
    "if the task is to predict the entry at a particular index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39023042",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e73f5f",
   "metadata": {},
   "source": [
    "Suppose, we were working with natural text where the tokens were words. To feed in a word like \"apple\" to a neural network, we need to \"numericalize\" (i.e. convert it to a number) it.\n",
    "\n",
    "The simplest solution is to map each unique token to a unique integer. For example:\n",
    "\n",
    "\"apple\" -> 0\n",
    "\n",
    "\"is\" -> 1\n",
    "\n",
    "\"a\" -> 2\n",
    "\n",
    "etc.\n",
    "\n",
    "Note that the only requirement is that this mapping is one-to-one i.e. different words are mapped to different integers. The actual mapping, i.e. whether \"apple\" -> 0 or \"apple\" -> 59, doesn't matter.\n",
    "\n",
    "Are there any problems with this encoding of tokens? One immediate problem is that it imposes an ordering on the tokens. \"apple\" is not less than \"a\" but 2 < 0. Depending on the machine learning model used, this ordered encoding can induce artifacts that are not real.\n",
    "\n",
    "The solution to this problem is the so-called one-hot encoding. Suppose, there are N distinct words. Each word is mapped to a vector of size N where exactly one entry is 1 and the rest are zeros. Suppose, N = 3. Then,\n",
    "\n",
    "\"apple\" -> [1,0,0]\n",
    "\n",
    "\"is\" -> [0,1,0]\n",
    "\n",
    "\"a\" -> [0,0,1]\n",
    "\n",
    "Now, there is no order imposed on the tokens. Each vector is orthogonal to all other vectors (the dot product of vectors corresponding to distinct words is 0). This creates a couple of problems. If the number of tokens is large, the dimensionality N will also be large and this has implications for memory usage. Another problem is more conceptual. While each word is distinct (by definition) from every other word, words are not distinct by meaning. \"apple\" and \"mango\" are similar in the sense that they are both fruits but they are clearly also distinct (winter vs summer fruit etc.). Since vectors can be used to encode similarity, is it possible to map tokens to vectors such that (a) similar meaning words map to similar vectors and (b) dissimilar meaning words map to dissimilar vectors.\n",
    "\n",
    "This is the problem embeddings solve. The philosophy in neural networks is to map each token to a unique vector is a relatively small (compared to the number of distinct tokens, N) dimensional (128 below) vector space. The embeddings are initialized randomly but are also adjusted during the learning process using the same exact process used to adjust/learn weights i.e. by computing derivatives and using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1666d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "emb = nn.Embedding(num_embeddings=high-low, embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1a38a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5423,  0.5266,  1.1882,  0.9355, -0.7791,  0.1069,  1.1231,  1.6374,\n",
       "         0.9743, -0.0636, -0.9660, -0.9154,  0.8699, -0.0161, -0.0410,  1.1897,\n",
       "        -0.4487, -0.0157,  0.3108,  0.1782, -1.2999,  0.4343,  0.0879, -0.3289,\n",
       "        -0.3270,  2.1025,  0.3177,  1.0361, -1.3813, -2.7314, -0.8861,  0.8975,\n",
       "        -0.6807, -0.0059,  1.8005, -0.3465, -0.2582,  0.1157, -1.1240,  1.2524,\n",
       "        -0.8958,  0.4613, -0.9092,  0.6420,  0.2388, -0.3712, -0.2952,  1.2677,\n",
       "        -0.7731,  0.8679, -0.1787,  0.5150,  0.4532, -1.6344,  1.3339,  0.9072,\n",
       "         0.3769, -2.1367,  1.0948, -0.8444, -0.1842,  0.2181,  0.4649, -1.4071,\n",
       "        -0.8272, -0.3051,  1.0011, -0.7690,  0.1095, -0.3177, -0.6874,  0.5006,\n",
       "         0.5810,  1.7755, -0.2017, -0.6410,  0.4117,  0.4148,  2.4842, -0.9477,\n",
       "        -0.2940,  1.2848, -1.1068, -0.3601,  0.1138,  0.6132,  0.2530, -1.5089,\n",
       "         1.7862,  0.4336, -0.0694, -0.5198, -0.3226, -0.2274, -0.4900,  1.7660,\n",
       "        -1.2420,  0.1898,  2.2897, -0.3930,  0.9352,  0.8136,  0.8144,  0.7611,\n",
       "         0.4444, -0.7189,  0.4808, -0.6523,  0.4509, -0.2086, -0.5353,  0.1134,\n",
       "        -0.9616,  0.1787, -0.8034, -0.2474,  1.2911, -0.5711,  0.8714, -0.1611,\n",
       "        -0.2028, -0.1894,  0.9272,  0.2722,  0.1121,  0.3192, -1.8900,  1.1281],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can now look up embedding vectors based on input token (any value between low and high-1)\n",
    "\n",
    "emb(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7ad8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2355,  0.6564, -1.9059,  2.4502,  1.7588, -0.5307,  0.7851, -0.6040,\n",
       "        -1.4562, -0.4593,  2.6170,  0.4103,  2.0874, -0.9793,  0.8330,  1.4915,\n",
       "        -1.3457,  0.5617, -0.6056,  0.6687, -0.5855,  1.0226, -1.8398, -0.1697,\n",
       "         0.1374,  1.5272,  1.5056,  0.4490,  0.9732,  0.9570, -0.8251, -1.0611,\n",
       "         0.3797,  0.3602, -0.1405, -0.5662, -1.0502,  1.2331,  1.0156,  0.1985,\n",
       "        -1.8225,  0.0731,  0.8447,  0.3947,  2.4693, -2.5529,  0.8676, -1.1341,\n",
       "         1.0207, -1.0664,  0.1900, -0.3369, -0.1403,  0.7167, -1.7084,  1.4050,\n",
       "        -0.2450, -0.4552,  1.0088,  0.4901,  2.0871, -1.9398, -0.3584, -1.6027,\n",
       "        -0.8999, -2.7576,  0.0708,  1.5830,  0.5663,  0.8460,  1.4456,  0.1762,\n",
       "         1.0485,  1.1570,  0.4039, -1.2401,  0.8021,  0.4174,  1.3183, -0.0089,\n",
       "         0.0594, -0.9420,  1.1330, -1.2755,  0.6854, -0.2580,  0.8663, -0.4360,\n",
       "        -0.9288, -0.5829,  0.3857,  0.5792,  1.4026,  0.4559, -0.0683,  0.6871,\n",
       "         0.9299,  1.0921, -1.8848, -0.1799, -0.4461,  0.3223, -1.5591,  0.3391,\n",
       "         0.9362, -0.2736,  1.2662, -0.5775,  0.1665,  1.8401,  0.7958,  2.6017,\n",
       "        -0.5868,  0.0865, -0.7146, -1.6882, -0.3492,  2.3581, -0.7953, -0.4167,\n",
       "        -0.6705,  0.6147, -0.2404, -0.1760, -0.6147, -1.0427,  0.8314,  0.7377],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor(high-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14434c89",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1965160/3155071257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#only values between low and high-1 have entries in the table\n",
    "emb(torch.tensor(high))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb17fa",
   "metadata": {},
   "source": [
    "### RNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68cd3c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4090396775.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1965160/4090396775.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class Net(nn.Module):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.rnn = nn.LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ab2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
