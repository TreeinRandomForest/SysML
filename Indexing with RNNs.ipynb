{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa00763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def gen_data(N=100, d=10, low=0, high=10):\n",
    "    data = np.random.randint(low=low, high=high, size=(N,d))\n",
    "    \n",
    "    target_idx = np.random.randint(low=0, high=d, size=N).reshape(data.shape[0], 1)\n",
    "    \n",
    "    full_data = np.hstack((data, target_idx))\n",
    "    \n",
    "    y = data[np.arange(data.shape[0]), target_idx[:,0]]\n",
    "    \n",
    "    return full_data, y\n",
    "\n",
    "N = 5000\n",
    "low = 0 \n",
    "high = 10\n",
    "d = 10\n",
    "\n",
    "train_data, train_target = gen_data(N=N, low=low, high=high, d=d)\n",
    "test_data, test_target = gen_data(N=N, low=low, high=high, d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bfecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 11)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape) #last column is the actual index value and not part of the array\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bf706",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26218",
   "metadata": {},
   "source": [
    "An RNN will process a sequence of tokens. The pseudocode is something like the following:\n",
    "\n",
    "token_list = [...]\n",
    "\n",
    "hidden_vec = [0, ..., 0] #some fixed length or dimensionality\n",
    "\n",
    "for token in token_list:\n",
    "\n",
    "#lookup vector for each token from a hash table\n",
    "    token_vec = embedding_table[token]\n",
    "    \n",
    "    #use previous hidden_vec and current token_vec to update hidden_vec\n",
    "    #this is updating the state (hidden_vec) of the net using the new token\n",
    "    \n",
    "    hidden_vec = update(hidden_vec/previous state, token_vec/new data)\n",
    "    \n",
    "after the loop, hidden_vec encodes all the information about the input sequence and can be used to make a prediction\n",
    "\n",
    "prediction = pred(hidden_vec)\n",
    "\n",
    "for us, this could also be\n",
    "\n",
    "prediction = pred(hidden_vec, index)\n",
    "\n",
    "if the task is to predict the entry at a particular index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5079b",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2afe",
   "metadata": {},
   "source": [
    "Suppose, we were working with natural text where the tokens were words. To feed in a word like \"apple\" to a neural network, we need to \"numericalize\" (i.e. convert it to a number) it.\n",
    "\n",
    "The simplest solution is to map each unique token to a unique integer. For example:\n",
    "\n",
    "\"apple\" -> 0\n",
    "\n",
    "\"is\" -> 1\n",
    "\n",
    "\"a\" -> 2\n",
    "\n",
    "etc.\n",
    "\n",
    "Note that the only requirement is that this mapping is one-to-one i.e. different words are mapped to different integers. The actual mapping, i.e. whether \"apple\" -> 0 or \"apple\" -> 59, doesn't matter.\n",
    "\n",
    "Are there any problems with this encoding of tokens? One immediate problem is that it imposes an ordering on the tokens. \"apple\" is not less than \"a\" but 2 < 0. Depending on the machine learning model used, this ordered encoding can induce artifacts that are not real.\n",
    "\n",
    "The solution to this problem is the so-called one-hot encoding. Suppose, there are N distinct words. Each word is mapped to a vector of size N where exactly one entry is 1 and the rest are zeros. Suppose, N = 3. Then,\n",
    "\n",
    "\"apple\" -> [1,0,0]\n",
    "\n",
    "\"is\" -> [0,1,0]\n",
    "\n",
    "\"a\" -> [0,0,1]\n",
    "\n",
    "Now, there is no order imposed on the tokens. Each vector is orthogonal to all other vectors (the dot product of vectors corresponding to distinct words is 0). This creates a couple of problems. If the number of tokens is large, the dimensionality N will also be large and this has implications for memory usage. Another problem is more conceptual. While each word is distinct (by definition) from every other word, words are not distinct by meaning. \"apple\" and \"mango\" are similar in the sense that they are both fruits but they are clearly also distinct (winter vs summer fruit etc.). Since vectors can be used to encode similarity, is it possible to map tokens to vectors such that (a) similar meaning words map to similar vectors and (b) dissimilar meaning words map to dissimilar vectors.\n",
    "\n",
    "This is the problem embeddings solve. The philosophy in neural networks is to map each token to a unique vector is a relatively small (compared to the number of distinct tokens, N) dimensional (128 below) vector space. The embeddings are initialized randomly but are also adjusted during the learning process using the same exact process used to adjust/learn weights i.e. by computing derivatives and using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "emb = nn.Embedding(num_embeddings=high-low, embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can now look up embedding vectors based on input token (any value between low and high-1)\n",
    "\n",
    "emb(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(torch.tensor(high-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only values between low and high-1 have entries in the table\n",
    "emb(torch.tensor(high))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53ca4f",
   "metadata": {},
   "source": [
    "### RNN definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e9cd2",
   "metadata": {},
   "source": [
    "The code below defines the recurrent neural network. There are three broad classes of RNNs:\n",
    "\n",
    "1. Vanilla RNNs - these tend to have a problem learning long-range behavior if the sequences are long. This is due to the so-called exploding and vanishing gradients problem (don't worry about this for now).\n",
    "\n",
    "2. Long short-term memory networks (LSTMs): instead of having just one (hidden) state like RNNs, LSTMs maintain a long-term memory vector and a short-term memory vector. The coarse idea is to use the long-term memory vector to \"remember\" long-range patterns.\n",
    "\n",
    "3. Gated Recurrent Units (GRUs): A simpler (less parameters/weights) form of LSTMs with the same underlying idea.\n",
    "\n",
    "(We can go over the details in a call)\n",
    "\n",
    "We will also use \"attention\". The core idea of attention is described below.\n",
    "\n",
    "As an RNN processes an input sequence (sentence or byte vector), it generates a sequence of hidden vectors.\n",
    "\n",
    "$$h_1, h_2, \\ldots, h_T$$\n",
    "\n",
    "where T = length of sequence.\n",
    "\n",
    "The final hidden state is then used as a measure of context for any downstream tasks (predicting an output sequence or predicting a class for the sequence).\n",
    "\n",
    "Attention refers to the idea that the context shouldn't consist just of $h_T$ but should be dynamic/flexible. \n",
    "\n",
    "For more details, either see the code below or the notes on attention near the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9326fd0",
   "metadata": {},
   "source": [
    "#### First, experiment with an LSTM to understand shapes of various tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "rnn = nn.LSTM(input_size = 128, #dimension of embedding\n",
    "              hidden_size = 32, #\n",
    "              num_layers = 2,\n",
    "              batch_first = True, #expect (batch, seq, feature)\n",
    "              dropout = 0.5,\n",
    "              bidirectional=True                             \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to understand data flow. pick some input data\n",
    "inp = torch.from_numpy(train_data[0:5, :-1])\n",
    "inp.shape #(batch/sequence number, length/time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(inp).shape #(batch/sequence number, length/time, embedding feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157368d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rnn(emb(inp))))\n",
    "len(rnn(emb(inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rnn(emb(inp))[0]))\n",
    "print(rnn(emb(inp))[0].shape) #(batch/sequence number, length/time, embedding feature*2 for bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rnn(emb(inp))[1]))\n",
    "print(len(rnn(emb(inp))[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rnn(emb(inp))[1][0]))\n",
    "print(rnn(emb(inp))[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445819d",
   "metadata": {},
   "source": [
    "Why is the output above of shape (4,5,32)?\n",
    "\n",
    "32 is the hidden dim i.e. the dimensionality of the hidden state and the cell state in an LSTM\n",
    "\n",
    "5 is the number of sequences in the batch (if this is not convincing, try changing inp to have, say, 7 sequences)\n",
    "\n",
    "where does the 4 come from? Claim: The 4 = 2 (num_layers) * 2 (bidirectional) cell states\n",
    "\n",
    "Of course, we could have looked at the documentation:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "Outputs: outputs, (h_n, c_n)\n",
    "\n",
    "where \n",
    "\n",
    "outputs.shape = (number of examples, length of sequence, 2*hidden_dim)\n",
    "\n",
    "h_n.shape = (2*num_layers, number of examples, hidden_dim)\n",
    "\n",
    "c_n.shape = (2*num_layers, number of examples, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rnn(emb(inp))[1][1]))\n",
    "print(rnn(emb(inp))[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1981de",
   "metadata": {},
   "source": [
    "**Mock run**:\n",
    "    \n",
    "It's generally a good idea to pick one training point (X, y pair) and run through the operations manually before defining the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: look at data\n",
    "\n",
    "idx = 5\n",
    "\n",
    "X = train_data[idx, :]\n",
    "y = train_target[idx]\n",
    "\n",
    "X1 = X[:-1]\n",
    "X2 = X[-1]\n",
    "\n",
    "print(X1, X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: compute embeddings for one element of input sequence\n",
    "low = 0\n",
    "high = 10\n",
    "\n",
    "embedding_dim = 128\n",
    "emb = nn.Embedding(num_embeddings=high-low, embedding_dim=128)\n",
    "\n",
    "emb(torch.from_numpy(X1)[3]) #can get embeddings for each element of X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8287009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: compute hidden states for one step of input sequence\n",
    "o, (h_n,c_n) = rnn(emb(torch.from_numpy(X1)[3])[None,None,:]) #emb()[None, None, :] of shape (1,1,embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8813a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.shape)\n",
    "print(h_n.shape)\n",
    "print(c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: compute over full sequence\n",
    "print('raw:', torch.from_numpy(X1).shape) #(length of sequence)\n",
    "print('raw unsqueeze:', torch.from_numpy(X1)[None, :].shape) #(1, length of sequence) where 1 is the number of sequences\n",
    "print('emb:', emb(torch.from_numpy(X1)[None, :]).shape) #(1, length of sequence, embedding dim/input dim for rnn)\n",
    "\n",
    "o, (h,c) = rnn(emb(torch.from_numpy(X1)[None, :]))\n",
    "print('o:', o.shape) #(num of sequences = 1, length of sequence, 2 (bidirectional)*hidden_dim) #last layer, at every time-step\n",
    "print('h:', h.shape) #(bidirection*num_layers, num of sequences = 1, hidden_dim) - at last time-step\n",
    "print('c:', c.shape) #(bidirection*num_layers, num of sequences = 1, hidden_dim) - at last time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5: another embedding layer for index\n",
    "#note: it would make sense for us to emb the position of each element in the input sequence and match the embedding\n",
    "#of the index to the embeddings of the position\n",
    "#note 2: if instead, we were doing a \"semantic search\", then embeddings of the search token would make more sense\n",
    "\n",
    "\n",
    "\n",
    "emb_idx = nn.Embedding(num_embeddings=d, embedding_dim=128) #there are d unique index values\n",
    "\n",
    "emb_idx(torch.tensor(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6: learn a dense layer to map embedding of idx and compare to output values for a sequence\n",
    "\n",
    "attn_lin = nn.Linear(128, 64) #128 is embedding_dim, 64 is bidirectional * hidden_dim (32)\n",
    "attn_lin(emb_idx(torch.tensor(X2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3aa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7: compute attention scores\n",
    "\n",
    "#7a: compute dot product of attn_lin(emb_idx()) with output\n",
    "\n",
    "a = attn_lin(emb_idx(torch.tensor(X2))).unsqueeze(0) #in general, will be of shape (num sequences, 64)\n",
    "print('a:', a.shape)\n",
    "\n",
    "print('o:', o.shape)\n",
    "energies = torch.tensordot(a, o, dims=([1], [2]))\n",
    "print('energies:', energies.shape)\n",
    "energies = energies.squeeze(1)\n",
    "print('energies:', energies.shape) #(num sequences, length of sequence)\n",
    "print(energies)\n",
    "\n",
    "#7b: compute attention scores\n",
    "scores = torch.exp(energies)\n",
    "scores /= scores.sum(dim=1)\n",
    "print('scores:', scores.shape)\n",
    "print(scores)\n",
    "print(scores.sum())\n",
    "\n",
    "assert(torch.abs(scores.sum()-1.0) < 1e-5)\n",
    "\n",
    "out = torch.tensordot(o, scores, dims=([1], [1])).squeeze(2) #weighted hidden state used to make predictions\n",
    "print('out:', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3634367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 8: use out as input to small MLP to predict actual output\n",
    "\n",
    "out_layer = nn.Linear(64, high-low)\n",
    "out_activation = nn.Softmax(dim=1)\n",
    "out_activation(out_layer(out)) #actual probability that input_seq[input_idx] takes each value between high-low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf7f66b",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "It is time to put these 8 steps together in one architecture. It's good practice to put assertions and sanity checks since we are composing many operations together.\n",
    "\n",
    "The following cell generalizes the above calculation to a batch of data\n",
    "\n",
    "**Exercise**: Go over each line, run it, make sure you understand why the shapes are what they are and how it corresponds to the attention mechanism described towards the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e965ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: pick a batch of 5 examples\n",
    "X1 = train_data[4:9, :-1]\n",
    "X2 = train_data[4:9, -1]\n",
    "y = train_target[4:9]\n",
    "\n",
    "print('\\nData:')\n",
    "print(X1, X2, y)\n",
    "print(X1.shape, X2.shape, y.shape)\n",
    "\n",
    "#step 2: data -> embddings -> LSTM -> get output activations (from final layer, for all time-steps)\n",
    "o, (h,c) = rnn(emb(torch.from_numpy(X1)))\n",
    "print('\\nrnn outputs:')\n",
    "print('o:', o.shape) #(num of sequences = 1, length of sequence, 2 (bidirectional)*hidden_dim) #last layer, at every time-step\n",
    "print('h:', h.shape) #(bidirection*num_layers, num of sequences = 1, hidden_dim) - at last time-step\n",
    "print('c:', c.shape) #(bidirection*num_layers, num of sequences = 1, hidden_dim) - at last time-step\n",
    "\n",
    "#step 3: get embedding for index and use a linear layer to map it to 64 dimensions to compare to o\n",
    "emb_idx = nn.Embedding(num_embeddings=d, embedding_dim=128) #there are d unique index values\n",
    "\n",
    "attn_lin = nn.Linear(128, 64) #128 is embedding_dim, 64 is bidirectional * hidden_dim (32)\n",
    "\n",
    "\n",
    "#step 4: compute attention scores\n",
    "a = attn_lin(emb_idx(torch.tensor(X2))).unsqueeze(0) #in general, will be of shape (num sequences, 64)\n",
    "a = a.permute(1,2,0) #change shape so we can multiply it\n",
    "print('\\na:', a.shape)\n",
    "\n",
    "print('o:', o.shape)\n",
    "energies = o @ a #batch matrix multiplication (torch.bmm)\n",
    "print('energies:', energies.shape) #(num sequences, length of sequence, 1)\n",
    "\n",
    "#compute $\\alpha_{tj} i.e. probabilities\n",
    "scores = torch.exp(energies)\n",
    "scores = (scores / scores.sum(dim=1).unsqueeze(1))\n",
    "print('scores:', scores.shape)\n",
    "\n",
    "assert(scores.sum(dim=1).mean()==1) #by construction, should get a distribution over time-steps\n",
    "\n",
    "out = (o * scores).sum(dim=1) #compute the linear combination (weighted average)\n",
    "print('out:', out.shape)\n",
    "\n",
    "#step 5: map the attention-weighted hidden activations to prob distribution over discrete output space\n",
    "out_layer = nn.Linear(64, high-low)\n",
    "out_activation = nn.Softmax(dim=1)\n",
    "out_activation(out_layer(out)) #actual probability that input_seq[input_idx] takes each value between high-low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34c4aa",
   "metadata": {},
   "source": [
    "Now, it really is time to put it all together and define out neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ca8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand((64,10,1)).sum(dim=1).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17cc47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, low, high, emb_dim, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=high-low, embedding_dim=emb_dim)\n",
    "        self.emb_idx = nn.Embedding(num_embeddings=high-low, embedding_dim=emb_dim)\n",
    "        self.attn_lin = nn.Linear(emb_dim, hidden_size*2) #128 is embedding_dim, 64 is bidirectional * hidden_dim (32)\n",
    "        self.out_layer = nn.Linear(hidden_size*2, high-low)\n",
    "        \n",
    "        #see: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.rnn = nn.LSTM(input_size = emb_dim,\n",
    "                           hidden_size = hidden_size,\n",
    "                           num_layers = num_layers,\n",
    "                           batch_first = True, #expect (batch, seq, feature)\n",
    "                           dropout = 0.5,\n",
    "                           bidirectional=True                             \n",
    "                          )\n",
    "        \n",
    "        self.pred = nn.Linear(2*hidden_size, high-low)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        X1 = x[:, :-1]\n",
    "        X2 = x[:, -1]\n",
    "        \n",
    "        o, (h,c) = self.rnn(self.emb(X1))\n",
    "        \n",
    "        #attention - map index to appropriate dimensionality\n",
    "        a = self.attn_lin(self.emb_idx(X2)).unsqueeze(0)\n",
    "        a = a.permute(1,2,0) #change shape so we can multiply it\n",
    "        \n",
    "        #attention - compute energies (log probabilities)\n",
    "        energies = o @ a #batch matrix multiplication (torch.bmm)\n",
    "\n",
    "        #attention - compute scores\n",
    "        #scores = torch.exp(energies)\n",
    "        #scores = (scores / scores.sum(dim=1).unsqueeze(1))\n",
    "        scores = nn.Softmax(dim=1)(energies)\n",
    "\n",
    "        #print(scores.shape)\n",
    "        if torch.abs(scores.sum(dim=1).mean()-1) > 1e-8: #by construction, should get a distribution over time-steps\n",
    "            print(scores.sum(dim=1).mean())\n",
    "            print(scores.sum(dim=1))\n",
    "            import ipdb\n",
    "            ipdb.set_trace()\n",
    "            \n",
    "        #attention - linear combination of encoder hidden states\n",
    "        out = (o * scores).sum(dim=1)\n",
    "        out = self.out_layer(out) #logits\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ad076d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "model = Net(low, high, emb_dim, hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb89f81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data[4:9, :-1]\n",
    "y = train_target[4:9]\n",
    "\n",
    "model(torch.from_numpy(X)).shape #logits over output space for each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ab5eb",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5e9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Array(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        X = torch.from_numpy(self.data[i])\n",
    "        y = torch.tensor(self.targets[i])\n",
    "        \n",
    "        assert(X[X[-1]]==y)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4bdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Array(train_data, train_target)\n",
    "ds_test = Array(test_data, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b91907",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=64)\n",
    "dl_test = DataLoader(ds_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e54bfee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "a,b = next(iter(dl_train))\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a15ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "model = Net(low, high, emb_dim, hidden_size, num_layers=num_layers)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(net, dl_train, dl_test, n_epochs, criterion, optimizer, device, print_freq=10):\n",
    "    net.train()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        for idx, (X, y) in enumerate(dl_train):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = net(X)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(net.parameters(), 10)\n",
    "            for p in net.parameters():\n",
    "                p.register_hook(lambda g: torch.clamp(g, -10, 10))\n",
    "            \n",
    "            grad_mean = check_grads(net)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        if i % print_freq==0:\n",
    "            train_acc = validate(net, dl_train)\n",
    "            test_acc = validate(net, dl_test)\n",
    "            \n",
    "            #print(f'grad_mean = {grad_mean}')\n",
    "            print(f'train acc = {train_acc} test acc = {test_acc}')\n",
    "            net = net.train()\n",
    "    return net\n",
    "\n",
    "def validate(net, dl):\n",
    "    net.eval()\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    with torch.no_grad(): #don't store gradients since we are not training here\n",
    "        for i, (X,y) in enumerate(dl):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = net(X) \n",
    "            \n",
    "            #don't want just probabilities. pick class with maximum probability\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            \n",
    "            n_correct += (pred==y).sum()\n",
    "            n_total += pred.shape[0]\n",
    "            \n",
    "    return n_correct/n_total\n",
    "\n",
    "def check_grads(net):\n",
    "    grads = []\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.mean().detach().cpu().item())\n",
    "            \n",
    "    return np.mean(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1f6b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.4177999794483185 test acc = 0.3903999924659729\n",
      "train acc = 0.5740000009536743 test acc = 0.5442000031471252\n",
      "train acc = 0.722599983215332 test acc = 0.7008000016212463\n",
      "train acc = 0.8736000061035156 test acc = 0.863599956035614\n",
      "train acc = 0.9651999473571777 test acc = 0.9609999656677246\n",
      "train acc = 0.9983999729156494 test acc = 0.996999979019165\n",
      "train acc = 0.9835999608039856 test acc = 0.983199954032898\n",
      "train acc = 0.9959999918937683 test acc = 0.9959999918937683\n",
      "train acc = 0.9959999918937683 test acc = 0.9957999587059021\n"
     ]
    }
   ],
   "source": [
    "model = train(model, dl_train, dl_test, 10, criterion, optimizer, device=device, print_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c4bae",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239a310",
   "metadata": {},
   "source": [
    "Seq2Seq (sequence to sequence) nets map an arbitrary length input sequence to an arbitrary length output sequence. If one wanted to map an arbitrary length input sequence to an output sequence of the same length, then one can use one RNN/LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb96044",
   "metadata": {},
   "source": [
    "### Seq2Seq paper: https://arxiv.org/pdf/1409.3215.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a8e85",
   "metadata": {},
   "source": [
    "**Core idea**:\n",
    "\n",
    "* Use LSTM (encoder) with multiple layers (\"deep\") to map input sequence -> vector of fixed dimensionality.\n",
    "\n",
    "* Use another LSTM (decoder) with multiple layers to generate output sequence starting from vector of fixed dimensionality produced by encoder.\n",
    "\n",
    "* Additional trick that improves performance significantly: feed both the input sequence and reversed input sequence (bidirectional) to the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e6564",
   "metadata": {},
   "source": [
    "**Core idea figure**:\n",
    "\n",
    "![Seq2Seq](seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefc4e6",
   "metadata": {},
   "source": [
    "**Example applications where input/output values are sequences of varying lengths**:\n",
    "\n",
    "* Speech recognition: audio -> text\n",
    "\n",
    "* Machine translation: text -> text\n",
    "\n",
    "* Question answering: text -> text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ce797",
   "metadata": {},
   "source": [
    "**How to think about the encoder-decoder architecture**:\n",
    "\n",
    "Encoder: Read input tokens one at a time and generate compressed context vector of fixed dimensionality.\n",
    "\n",
    "Decoder: Conditioned on context vector, generate sequences. In other words, all information about the input sequence is passed to the decoder through the context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dcf12",
   "metadata": {},
   "source": [
    "**Model architecture**:\n",
    "\n",
    "* LSTMs instead of RNNs\n",
    "\n",
    "* Two different LSTMs are used i.e. the same LSTM weights are not used for the encoder and the decoder\n",
    "\n",
    "* Multi-layer LSTMs are used: 4 layers\n",
    "\n",
    "* Order of tokens/words is reversed in input sequence i.e. instead of mapping: a,b,c -> $\\alpha,\\beta,\\gamma$, map c,b,a -> $\\alpha, \\beta, \\gamma$ so that a is closer to $\\alpha$, b is closer to $\\beta$. But note that now c is further away from $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842424f",
   "metadata": {},
   "source": [
    "**Dataset details**: To get a sense of scale\n",
    "\n",
    "Trained on 12 million sentences\n",
    "\n",
    "Total unique French words: 384 million\n",
    "\n",
    "Total unique English words: 304 million\n",
    "\n",
    "For predictions, output space of words was restricted to 160k words in the source language (English) and 80k words in the target language (French). If word not in vocabulary, replace by special unknown token, UNK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3f938",
   "metadata": {},
   "source": [
    "**Training details**:\n",
    "\n",
    "loss = $\\frac{1}{\\mid \\mathcal{S} \\mid} \\Sigma_{(T,S)\\in\\mathcal{S}} \\log p(T|S)$\n",
    "\n",
    "where $\\mathcal{S}$ is the training set.\n",
    "\n",
    "In other words, for each training example, the source sentence S is used to predict the target sentence T. For each token in the prediction, we know the actual/label token that should have been predicted. Compute the log probability for the label token and add them up over the target sentence tokens. Average these across each example in the training set.\n",
    "\n",
    "* LSTMs: 4 layers, 1000 cells (hidden_dim), 1000 dimensional word embeddings (embedding_dim), input vocabulary size = 160k, output vocabulary size = 80k.\n",
    "\n",
    "* LSTM weights ~ uniform(-0.08, 0.08)\n",
    "\n",
    "* SGD without momentum, lr = 0.7 for first 5 epochs and halve lr every half epoch\n",
    "\n",
    "* Total training time = 7.5 epochs\n",
    "\n",
    "* Batch size = 128\n",
    "\n",
    "* For each batch, compute $s = \\Vert g \\Vert_2 / 128$ where g = gradient. If $s > 5$, set $g = \\frac{5g}{s}$ i.e. if length of vector exceeds 5, rescale to make length = 5.\n",
    "\n",
    "* Most sentences are short (20-30 tokens) and some are long (>100 tokens). Make sure sentences in each batch are roughly the same length to avoid wasted computation.\n",
    "\n",
    "* Use model parallelism with different layers on different GPUs. Note this paper was written before TensorFlow, PyTorch, Theano (?) etc.\n",
    "\n",
    "**Prediction/inference details**:\n",
    "\n",
    "Beam search with size = 1 or 2 works well. In detail, the decoder is used to predict a probability distribution over all possible tokens at each time-step. Since the output of the decode at time t is the input to the decoder at time t+1, we need a sample from the distribution. \n",
    "\n",
    "Ideally, we want the output sequence to be such that the sum of the log probabilities of the sampled tokens is maximized. This is not the same as a greedy sampling strategy. Beam search keeps track of the top k running sums of log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e43358",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "* While training, how should the hidden state of the encoder be initialized?\n",
    "    * Zeros for each batch\n",
    "    * Make hidden state initialization learned?\n",
    "    * Keep hidden state evolving as batches get processed? This sounds troublesome since the implication is the order in which sequences are processed, matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8c9aa",
   "metadata": {},
   "source": [
    "### Seq2Seq + Attention paper: https://arxiv.org/pdf/1409.0473.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea78baf",
   "metadata": {},
   "source": [
    "**Core idea**:\n",
    "\n",
    "* In the encoder-decoder setup, some tokens in the decoder/output sequence are far away from the corresponding tokens in the encoder/input sequence.\n",
    "\n",
    "* Since all context about the input is compressed into the context vector, it might lose information about earlier tokens. In other words, the context vector can be an information bottleneck.\n",
    "\n",
    "* This manifests itself as poor performance when the length of the input sequence gets long (longer than training data sequences).\n",
    "\n",
    "* Can each token in the decoder be allowed to search for relevant tokens in the input sentence? This search has to be soft i.e. predict probabilities over the input sequence rather than hard choices.\n",
    "\n",
    "* In this paper, a mechanism (attention) is introduced where, at each time-step in the decoder, a soft search over all hidden states in the encoder is carried out. This search is used to generate a new appropriate context vector that focuses on subsets of the input sequence. In other words, there is no need to encode the full input sequence into *one* context vector. Instead each input sequence is encoded into a sequence of vectors and during decoding, a soft search is carried out over this sequence of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7edcce",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "While in the previous paper, the final hidden state in the decoder, $h_T$ is used as a context vector, as this paper suggests, one could generalize to:\n",
    "\n",
    "$$c = q({h_1, \\ldots, h_T})$$\n",
    "\n",
    "i.e. the context vector is some (fixed) function of all the hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc57bc",
   "metadata": {},
   "source": [
    "**Core idea figure**:\n",
    "\n",
    "![Model](seq2seq_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39db22c",
   "metadata": {},
   "source": [
    "The $x_t$ are the input tokens to the encoder. The $h_t$ are the hidden states (bidirectional). The key difference is the computation of the decoder's hidden state at time t, $s_t$.\n",
    "\n",
    "In the classic encoder-decoder picture, $s_t = f(s_{t-1}, y_{t-1})$. In this model,\n",
    "\n",
    "$$s_t = f(s_{t-1}, y_{t-1}, h_{1}, h_{2}, \\ldots, h_T)$$\n",
    "\n",
    "where $f()$ sloppily refers to \"some function\" (not the same one in both computations). So, now the computation has direct access to each encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9441aa2",
   "metadata": {},
   "source": [
    "**Precise formulation**:\n",
    "\n",
    "Given an input sequence S, of length T, the encoder produces hidden states, $h_1, \\ldots, h_T$.\n",
    "\n",
    "These hidden states are used to compute a context vector, $c$. In the seq2seq paper, $c = h_T$.\n",
    "\n",
    "The decoder conditions on the context vector i.e. it intializes its hidden state, $s_0$ so that $s_0 = c$.\n",
    "\n",
    "\n",
    "The first token is a special SOS (start of sentence) token and is passed as the first input $y_0$. The decoder hidden state is updated:\n",
    "\n",
    "$$s_1 = f(s_0, y_0)$$\n",
    "\n",
    "which is used to compute the probability distribution over the vocabulary:\n",
    "\n",
    "$$p(y_1\\mid y_0, c) = g(s_1)$$\n",
    "\n",
    "\n",
    "For any time t,\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1})}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}, c) = g(s_{t})}$$\n",
    "\n",
    "Both of these equations implictly depend on the context vector $c$ since every calculation depends on $s_0 = c$. We make this explicit:\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1}, c)}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}, c) = g(s_{t}, c)}$$\n",
    "\n",
    "One way of looking at attention is to make $c$ dependent on time t, i.e.:\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1}, c_t)}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}) = g(s_{t}, c_t)}$$\n",
    "\n",
    "\n",
    "This implies an order of computation:\n",
    "\n",
    "* Compute $c_t$ which is the context vector at time t.\n",
    "\n",
    "* Compute hidden state $s_t$ from $c_t, s_{t-1}, y_{t-1}$.\n",
    "\n",
    "* Compute prob distribution from $s_t, c_t$.\n",
    "\n",
    "The context vector, instead of being just $h_T$ (last hidden vector in the encoder) is now generalized to a linear (convex) combination of all encoder hidden vectors:\n",
    "\n",
    "$$c_t = \\Sigma_{j} \\alpha_{tj}h_j$$\n",
    "\n",
    "where $\\alpha_{tj}$ can be interpreted as probabilities as shown below.\n",
    "\n",
    "$$\\alpha_{tj} = \\frac{e^{e_{tj}}}{\\Sigma_{k}e^{e_{tk}}}$$\n",
    "\n",
    "This is essentially a Boltzmann probability or you can think of $e_{tj}$ as the energy of the $j$th configuration, the exponential ensures that $\\alpha_{tj} > 0$ and the denominator ensures the $\\alpha_{tj}$ add up to 1 (when summed over $j$). \n",
    "\n",
    "The next question is how $e_{tj}$ is computed and there are many choices here. In general, $e_{tj} = a(s_{t-1}, h_j)$. Note that $e_{tj}$ is computed before $s_t$ and hence depends on $s_{t-1}$ and not $s_t$. In words, \"based on what the decoder has generated so far till time t-1 and the summary encoded in $s_{t-1}$, what information can be gathered from the encoder's hidden states to compute the next hidden state $s_t$ so the next token can be computed\".\n",
    "\n",
    "**Open question**: what would happen if $s_t$ were used to compute $c_t$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b8d69",
   "metadata": {},
   "source": [
    "**Model architecture**:\n",
    "\n",
    "Encoder: Bidirectional RNN (LSTM etc.)\n",
    "\n",
    "Decoder: RNN (LSTM etc.) + Attention as described above\n",
    "\n",
    "Loss: multi-class log loss/cross-entropy\n",
    "\n",
    "Decoding strategy: Beam search\n",
    "\n",
    "Optimizer: Adadelta (training time for paper's model ~ 5 days on what hardware?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb97e3",
   "metadata": {},
   "source": [
    "**Model details**: (See appendix)\n",
    "\n",
    "**RNN**: Use LSTM or GRUs i.e. architectures that let one learn long-term dependencies since there are connections where the gradient is close to 1.\n",
    "\n",
    "Recall $s_t = f(s_{t-1}, y_{t-1}, c_t)$ with a time-dependent context.\n",
    "\n",
    "The precise computation used here is:\n",
    "\n",
    "$$s_t = f(s_{t-1}, y_{t-1}, c_t) = (1-z_t) \\odot s_{t-1} + z_t \\odot \\tilde{s}_t$$\n",
    "\n",
    "where $\\odot$ is element-wise multiplication, $z_t$ is the output of the update gates in the LSTM/GRU unit.\n",
    "\n",
    "One way to think of this computation is as follows:\n",
    "\n",
    "$z_t$ is a vector of numbers between 0 and 1 and hence is a soft mask or a vector of probabilities. It thereforce keeps elements of the previous state $s_{t-1}$ with probability $1-z_t$ and updates them with elements of a new hidden state candidate, $\\tilde{s}_t$ with probability $z_t$.\n",
    "\n",
    "The new candidate is defined to be:\n",
    "\n",
    "$$\\tilde{s}_t = \\tanh(W e(y_{t-1}) + U [r_t \\odot s_{t-1}] + C c_t)$$\n",
    "\n",
    "Note that apart from using the reset gate, $r_t$, this is the usual computation of a hidden unit where $e(y_{t-1})$ is the embedding of the token from time $t-1$. At its simplest, this embedding is just one-hot encoding.\n",
    "\n",
    "The two gates are computed using the same logic as any hidden recurrent unit:\n",
    "\n",
    "$$z_t = \\sigma(W_z e(y_{t-1}) + U_z s_{t-1} + C_z c_t)$$\n",
    "\n",
    "$$r_t = \\sigma(W_r e(y_{t-1}) + U_r s_{t-1} + C_r c_t)$$\n",
    "\n",
    "and $\\sigma$ is the sigmoid function to get values in range $(0,1)$.\n",
    "\n",
    "**Computation of c_t**:\n",
    "\n",
    "Recall that the context vector is a convex combination of the encoder's hidden states:\n",
    "\n",
    "$$c_t = \\Sigma_{j=1}^{T_x} \\alpha_{tj} h_j$$ \n",
    "\n",
    "where the weights $\\alpha_{tj}$ are computed using Boltzmann probabilities:\n",
    "\n",
    "$$\\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\Sigma_k \\exp(e_{tk})}$$\n",
    "\n",
    "and the energies, $e_{tj}$ are:\n",
    "\n",
    "$$e_{tj} = a(s_{t-1}, h_j)$$\n",
    "\n",
    "There are many choices for $a()$. The one made by this paper is:\n",
    "\n",
    "$$e_{tj} = a(s_{t-1}, h_j) = v_a^T \\tanh(W_a s_{t-1} + U_a h_j)$$\n",
    "\n",
    "Here, $v_a, W_a, U_a$ are learned parameters. You can think of $e_{tj}$ as a similarity score between $s_{t-1}$ and $h_j$. The matrices $W_a$ and $U_a$ map $s_{t-1}$ and $h_j$ respectively, to the same vector space so they can be added together. Note, another simple choice could be:\n",
    "\n",
    "$$e_{tj} = s_{t-1}^T W_a h_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756748c5",
   "metadata": {},
   "source": [
    "**Training details**: \n",
    "\n",
    "* All recurrent weight matrices are initialized as random orthogonal matrices ($U^T U = I$)\n",
    "\n",
    "* $W_a$ and $U_a$ have each element drawn from a gaussian distribution, $\\mathcal{N}(0, 0.001^2)$\n",
    "\n",
    "* Biases and $V_a$ were initialized to zero.\n",
    "\n",
    "* Any other weight matrices had elements drawn from $\\mathcal{N}(0, 0.01^2)$\n",
    "\n",
    "* Adadelta (adaptive SGD algorithm) was used with parameters ($\\epsilon = 10^{-6}$ and $\\rho = 0.95$)\n",
    "\n",
    "* Gradients were restricted to be at most of $L_2$ norm = 1\n",
    "\n",
    "* Batch size = 80 sentences\n",
    "\n",
    "* Every 20th update, 20*80 = 1600 sentences were retrieved, sorted by sequence length and split into 20 batches for the next 20 updates. This is because the time spent on a batch was proportional to the length of the longest sequence.\n",
    "\n",
    "* Training data shuffled once before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f16aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
