{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa00763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def gen_data(N=100, d=10, low=0, high=10, target_idx=3):\n",
    "    data = np.random.randint(low=low, high=high, size=(N,d))\n",
    "    return data, data[:, target_idx]\n",
    "\n",
    "N = 5000\n",
    "low = 0 \n",
    "high = 10\n",
    "train_data, train_target = gen_data(N=N, low=low, high=high)\n",
    "test_data, test_target = gen_data(N=N, low=low, high=high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bf706",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26218",
   "metadata": {},
   "source": [
    "An RNN will process a sequence of tokens. The pseudocode is something like the following:\n",
    "\n",
    "token_list = [...]\n",
    "\n",
    "hidden_vec = [0, ..., 0] #some fixed length or dimensionality\n",
    "\n",
    "for token in token_list:\n",
    "\n",
    "#lookup vector for each token from a hash table\n",
    "    token_vec = embedding_table[token]\n",
    "    \n",
    "    #use previous hidden_vec and current token_vec to update hidden_vec\n",
    "    #this is updating the state (hidden_vec) of the net using the new token\n",
    "    \n",
    "    hidden_vec = update(hidden_vec/previous state, token_vec/new data)\n",
    "    \n",
    "after the loop, hidden_vec encodes all the information about the input sequence and can be used to make a prediction\n",
    "\n",
    "prediction = pred(hidden_vec)\n",
    "\n",
    "for us, this could also be\n",
    "\n",
    "prediction = pred(hidden_vec, index)\n",
    "\n",
    "if the task is to predict the entry at a particular index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5079b",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2afe",
   "metadata": {},
   "source": [
    "Suppose, we were working with natural text where the tokens were words. To feed in a word like \"apple\" to a neural network, we need to \"numericalize\" (i.e. convert it to a number) it.\n",
    "\n",
    "The simplest solution is to map each unique token to a unique integer. For example:\n",
    "\n",
    "\"apple\" -> 0\n",
    "\n",
    "\"is\" -> 1\n",
    "\n",
    "\"a\" -> 2\n",
    "\n",
    "etc.\n",
    "\n",
    "Note that the only requirement is that this mapping is one-to-one i.e. different words are mapped to different integers. The actual mapping, i.e. whether \"apple\" -> 0 or \"apple\" -> 59, doesn't matter.\n",
    "\n",
    "Are there any problems with this encoding of tokens? One immediate problem is that it imposes an ordering on the tokens. \"apple\" is not less than \"a\" but 2 < 0. Depending on the machine learning model used, this ordered encoding can induce artifacts that are not real.\n",
    "\n",
    "The solution to this problem is the so-called one-hot encoding. Suppose, there are N distinct words. Each word is mapped to a vector of size N where exactly one entry is 1 and the rest are zeros. Suppose, N = 3. Then,\n",
    "\n",
    "\"apple\" -> [1,0,0]\n",
    "\n",
    "\"is\" -> [0,1,0]\n",
    "\n",
    "\"a\" -> [0,0,1]\n",
    "\n",
    "Now, there is no order imposed on the tokens. Each vector is orthogonal to all other vectors (the dot product of vectors corresponding to distinct words is 0). This creates a couple of problems. If the number of tokens is large, the dimensionality N will also be large and this has implications for memory usage. Another problem is more conceptual. While each word is distinct (by definition) from every other word, words are not distinct by meaning. \"apple\" and \"mango\" are similar in the sense that they are both fruits but they are clearly also distinct (winter vs summer fruit etc.). Since vectors can be used to encode similarity, is it possible to map tokens to vectors such that (a) similar meaning words map to similar vectors and (b) dissimilar meaning words map to dissimilar vectors.\n",
    "\n",
    "This is the problem embeddings solve. The philosophy in neural networks is to map each token to a unique vector is a relatively small (compared to the number of distinct tokens, N) dimensional (128 below) vector space. The embeddings are initialized randomly but are also adjusted during the learning process using the same exact process used to adjust/learn weights i.e. by computing derivatives and using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e905cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "emb = nn.Embedding(num_embeddings=high-low, embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cf9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5423,  0.5266,  1.1882,  0.9355, -0.7791,  0.1069,  1.1231,  1.6374,\n",
       "         0.9743, -0.0636, -0.9660, -0.9154,  0.8699, -0.0161, -0.0410,  1.1897,\n",
       "        -0.4487, -0.0157,  0.3108,  0.1782, -1.2999,  0.4343,  0.0879, -0.3289,\n",
       "        -0.3270,  2.1025,  0.3177,  1.0361, -1.3813, -2.7314, -0.8861,  0.8975,\n",
       "        -0.6807, -0.0059,  1.8005, -0.3465, -0.2582,  0.1157, -1.1240,  1.2524,\n",
       "        -0.8958,  0.4613, -0.9092,  0.6420,  0.2388, -0.3712, -0.2952,  1.2677,\n",
       "        -0.7731,  0.8679, -0.1787,  0.5150,  0.4532, -1.6344,  1.3339,  0.9072,\n",
       "         0.3769, -2.1367,  1.0948, -0.8444, -0.1842,  0.2181,  0.4649, -1.4071,\n",
       "        -0.8272, -0.3051,  1.0011, -0.7690,  0.1095, -0.3177, -0.6874,  0.5006,\n",
       "         0.5810,  1.7755, -0.2017, -0.6410,  0.4117,  0.4148,  2.4842, -0.9477,\n",
       "        -0.2940,  1.2848, -1.1068, -0.3601,  0.1138,  0.6132,  0.2530, -1.5089,\n",
       "         1.7862,  0.4336, -0.0694, -0.5198, -0.3226, -0.2274, -0.4900,  1.7660,\n",
       "        -1.2420,  0.1898,  2.2897, -0.3930,  0.9352,  0.8136,  0.8144,  0.7611,\n",
       "         0.4444, -0.7189,  0.4808, -0.6523,  0.4509, -0.2086, -0.5353,  0.1134,\n",
       "        -0.9616,  0.1787, -0.8034, -0.2474,  1.2911, -0.5711,  0.8714, -0.1611,\n",
       "        -0.2028, -0.1894,  0.9272,  0.2722,  0.1121,  0.3192, -1.8900,  1.1281],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can now look up embedding vectors based on input token (any value between low and high-1)\n",
    "\n",
    "emb(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d346de8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2355,  0.6564, -1.9059,  2.4502,  1.7588, -0.5307,  0.7851, -0.6040,\n",
       "        -1.4562, -0.4593,  2.6170,  0.4103,  2.0874, -0.9793,  0.8330,  1.4915,\n",
       "        -1.3457,  0.5617, -0.6056,  0.6687, -0.5855,  1.0226, -1.8398, -0.1697,\n",
       "         0.1374,  1.5272,  1.5056,  0.4490,  0.9732,  0.9570, -0.8251, -1.0611,\n",
       "         0.3797,  0.3602, -0.1405, -0.5662, -1.0502,  1.2331,  1.0156,  0.1985,\n",
       "        -1.8225,  0.0731,  0.8447,  0.3947,  2.4693, -2.5529,  0.8676, -1.1341,\n",
       "         1.0207, -1.0664,  0.1900, -0.3369, -0.1403,  0.7167, -1.7084,  1.4050,\n",
       "        -0.2450, -0.4552,  1.0088,  0.4901,  2.0871, -1.9398, -0.3584, -1.6027,\n",
       "        -0.8999, -2.7576,  0.0708,  1.5830,  0.5663,  0.8460,  1.4456,  0.1762,\n",
       "         1.0485,  1.1570,  0.4039, -1.2401,  0.8021,  0.4174,  1.3183, -0.0089,\n",
       "         0.0594, -0.9420,  1.1330, -1.2755,  0.6854, -0.2580,  0.8663, -0.4360,\n",
       "        -0.9288, -0.5829,  0.3857,  0.5792,  1.4026,  0.4559, -0.0683,  0.6871,\n",
       "         0.9299,  1.0921, -1.8848, -0.1799, -0.4461,  0.3223, -1.5591,  0.3391,\n",
       "         0.9362, -0.2736,  1.2662, -0.5775,  0.1665,  1.8401,  0.7958,  2.6017,\n",
       "        -0.5868,  0.0865, -0.7146, -1.6882, -0.3492,  2.3581, -0.7953, -0.4167,\n",
       "        -0.6705,  0.6147, -0.2404, -0.1760, -0.6147, -1.0427,  0.8314,  0.7377],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor(high-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7908585c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1965160/3155071257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/Downloads/venv_pytorch/lib64/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#only values between low and high-1 have entries in the table\n",
    "emb(torch.tensor(high))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53ca4f",
   "metadata": {},
   "source": [
    "### RNN definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e9cd2",
   "metadata": {},
   "source": [
    "The code below defines the recurrent neural network. There are three broad classes of RNNs:\n",
    "\n",
    "1. Vanilla RNNs - these tend to have a problem learning long-range behavior if the sequences are long. This is due to the so-called exploding and vanishing gradients problem (don't worry about this for now).\n",
    "\n",
    "2. Long short-term memory networks (LSTMs): instead of having just one (hidden) state like RNNs, LSTMs maintain a long-term memory vector and a short-term memory vector. The coarse idea is to use the long-term memory vector to \"remember\" long-range patterns.\n",
    "\n",
    "3. Gated Recurrent Units (GRUs): A simpler (less parameters/weights) form of LSTMs with the same underlying idea.\n",
    "\n",
    "(We can go over the details in a call)\n",
    "\n",
    "We will also use \"attention\". The core idea of attention is described below.\n",
    "\n",
    "As an RNN processes an input sequence (sentence or byte vector), it generates a sequence of hidden vectors.\n",
    "\n",
    "$$h_1, h_2, \\ldots, h_T$$\n",
    "\n",
    "where T = length of sequence.\n",
    "\n",
    "The final hidden state is then used as a measure of context for any downstream tasks (predicting an output sequence or predicting a class for the sequence).\n",
    "\n",
    "Attention refers to the idea that the context shouldn't consist just of $h_T$ but should be dynamic/flexible. \n",
    "\n",
    "To be finished - need to write more carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9326fd0",
   "metadata": {},
   "source": [
    "#### First, experiment with an LSTM to understand shapes of various tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70bd2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "rnn = nn.LSTM(input_size = 128, #dimension of embedding\n",
    "              hidden_size = 32, #\n",
    "              num_layers = 2,\n",
    "              batch_first = True, #expect (batch, seq, feature)\n",
    "              dropout = 0.5,\n",
    "              bidirectional=True                             \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e9e66eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#want to understand data flow. pick some input data\n",
    "inp = torch.from_numpy(train_data[0:5])\n",
    "inp.shape #(batch/sequence number, length/time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52f2e0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(inp).shape #(batch/sequence number, length/time, embedding feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "157368d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(rnn(emb(inp))))\n",
    "len(rnn(emb(inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b8283e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([5, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "print(type(rnn(emb(inp))[0]))\n",
    "print(rnn(emb(inp))[0].shape) #(batch/sequence number, length/time, embedding feature*2 for bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1356dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(rnn(emb(inp))[1]))\n",
    "print(len(rnn(emb(inp))[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d60c1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "print(type(rnn(emb(inp))[1][0]))\n",
    "print(rnn(emb(inp))[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445819d",
   "metadata": {},
   "source": [
    "Why is the output above of shape (4,5,32)?\n",
    "\n",
    "32 is the hidden dim i.e. the dimensionality of the hidden state and the cell state in an LSTM\n",
    "\n",
    "5 is the number of sequences in the batch (if this is not convincing, try changing inp to have, say, 7 sequences)\n",
    "\n",
    "where does the 4 come from? Claim: The 4 = 2 (num_layers) * 2 (bidirectional) cell states\n",
    "\n",
    "Of course, we could have looked at the documentation:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "Outputs: outputs, (h_n, c_n)\n",
    "\n",
    "where \n",
    "\n",
    "outputs.shape = (number of examples, length of sequence, 2*hidden_dim)\n",
    "\n",
    "h_n.shape = (2*num_layers, number of examples, hidden_dim)\n",
    "\n",
    "c_n.shape = (2*num_layers, number of examples, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2782f82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "print(type(rnn(emb(inp))[1][1]))\n",
    "print(rnn(emb(inp))[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17cc47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(low, high, emb_dim, hidden_size, num_layers=1)\n",
    "        \n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=high-low, embedding_dim=emb_dim)\n",
    "        \n",
    "        #see: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.rnn = nn.LSTM(input_size = embedding_dim,\n",
    "                           hidden_size = hidden_size,\n",
    "                           num_layers = num_layers,\n",
    "                           batch_first = True, #expect (batch, seq, feature)\n",
    "                           dropout = 0.5,\n",
    "                           bidirectional=True                             \n",
    "                          )\n",
    "        \n",
    "        self.pred = nn.Linear(2*hidden_size, high-low)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, (h_n, c_n) = self.rnn(x)\n",
    "        \n",
    "        #use the bidirectional hidden states from the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad076d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52cc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "471c4bae",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239a310",
   "metadata": {},
   "source": [
    "Seq2Seq (sequence to sequence) nets map an arbitrary length input sequence to an arbitrary length output sequence. If one wanted to map an arbitrary length input sequence to an output sequence of the same length, then one can use one RNN/LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb96044",
   "metadata": {},
   "source": [
    "### Seq2Seq paper: https://arxiv.org/pdf/1409.3215.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a8e85",
   "metadata": {},
   "source": [
    "**Core idea**:\n",
    "\n",
    "* Use LSTM (encoder) with multiple layers (\"deep\") to map input sequence -> vector of fixed dimensionality.\n",
    "\n",
    "* Use another LSTM (decoder) with multiple layers to generate output sequence starting from vector of fixed dimensionality produced by encoder.\n",
    "\n",
    "* Additional trick that improves performance significantly: feed both the input sequence and reversed input sequence (bidirectional) to the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e6564",
   "metadata": {},
   "source": [
    "**Core idea figure**:\n",
    "\n",
    "![Seq2Seq](seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefc4e6",
   "metadata": {},
   "source": [
    "**Example applications where input/output values are sequences of varying lengths**:\n",
    "\n",
    "* Speech recognition: audio -> text\n",
    "\n",
    "* Machine translation: text -> text\n",
    "\n",
    "* Question answering: text -> text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ce797",
   "metadata": {},
   "source": [
    "**How to think about the encoder-decoder architecture**:\n",
    "\n",
    "Encoder: Read input tokens one at a time and generate compressed context vector of fixed dimensionality.\n",
    "\n",
    "Decoder: Conditioned on context vector, generate sequences. In other words, all information about the input sequence is passed to the decoder through the context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dcf12",
   "metadata": {},
   "source": [
    "**Model architecture**:\n",
    "\n",
    "* LSTMs instead of RNNs\n",
    "\n",
    "* Two different LSTMs are used i.e. the same LSTM weights are not used for the encoder and the decoder\n",
    "\n",
    "* Multi-layer LSTMs are used: 4 layers\n",
    "\n",
    "* Order of tokens/words is reversed in input sequence i.e. instead of mapping: a,b,c -> $\\alpha,\\beta,\\gamma$, map c,b,a -> $\\alpha, \\beta, \\gamma$ so that a is closer to $\\alpha$, b is closer to $\\beta$. But note that now c is further away from $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842424f",
   "metadata": {},
   "source": [
    "**Dataset details**: To get a sense of scale\n",
    "\n",
    "Trained on 12 million sentences\n",
    "\n",
    "Total unique French words: 384 million\n",
    "\n",
    "Total unique English words: 304 million\n",
    "\n",
    "For predictions, output space of words was restricted to 160k words in the source language (English) and 80k words in the target language (French). If word not in vocabulary, replace by special unknown token, UNK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3f938",
   "metadata": {},
   "source": [
    "**Training details**:\n",
    "\n",
    "loss = $\\frac{1}{\\mid \\mathcal{S} \\mid} \\Sigma_{(T,S)\\in\\mathcal{S}} \\log p(T|S)$\n",
    "\n",
    "where $\\mathcal{S}$ is the training set.\n",
    "\n",
    "In other words, for each training example, the source sentence S is used to predict the target sentence T. For each token in the prediction, we know the actual/label token that should have been predicted. Compute the log probability for the label token and add them up over the target sentence tokens. Average these across each example in the training set.\n",
    "\n",
    "* LSTMs: 4 layers, 1000 cells (hidden_dim), 1000 dimensional word embeddings (embedding_dim), input vocabulary size = 160k, output vocabulary size = 80k.\n",
    "\n",
    "* LSTM weights ~ uniform(-0.08, 0.08)\n",
    "\n",
    "* SGD without momentum, lr = 0.7 for first 5 epochs and halve lr every half epoch\n",
    "\n",
    "* Total training time = 7.5 epochs\n",
    "\n",
    "* Batch size = 128\n",
    "\n",
    "* For each batch, compute $s = \\Vert g \\Vert_2 / 128$ where g = gradient. If $s > 5$, set $g = \\frac{5g}{s}$ i.e. if length of vector exceeds 5, rescale to make length = 5.\n",
    "\n",
    "* Most sentences are short (20-30 tokens) and some are long (>100 tokens). Make sure sentences in each batch are roughly the same length to avoid wasted computation.\n",
    "\n",
    "* Use model parallelism with different layers on different GPUs. Note this paper was written before TensorFlow, PyTorch, Theano (?) etc.\n",
    "\n",
    "**Prediction/inference details**:\n",
    "\n",
    "Beam search with size = 1 or 2 works well. In detail, the decoder is used to predict a probability distribution over all possible tokens at each time-step. Since the output of the decode at time t is the input to the decoder at time t+1, we need a sample from the distribution. \n",
    "\n",
    "Ideally, we want the output sequence to be such that the sum of the log probabilities of the sampled tokens is maximized. This is not the same as a greedy sampling strategy. Beam search keeps track of the top k running sums of log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e43358",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "* While training, how should the hidden state of the encoder be initialized?\n",
    "    * Zeros for each batch\n",
    "    * Make hidden state initialization learned?\n",
    "    * Keep hidden state evolving as batches get processed? This sounds troublesome since the implication is the order in which sequences are processed, matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8c9aa",
   "metadata": {},
   "source": [
    "### Seq2Seq + Attention paper: https://arxiv.org/pdf/1409.0473.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea78baf",
   "metadata": {},
   "source": [
    "**Core idea**:\n",
    "\n",
    "* In the encoder-decoder setup, some tokens in the decoder/output sequence are far away from the corresponding tokens in the encoder/input sequence.\n",
    "\n",
    "* Since all context about the input is compressed into the context vector, it might lose information about earlier tokens. In other words, the context vector can be an information bottleneck.\n",
    "\n",
    "* This manifests itself as poor performance when the length of the input sequence gets long (longer than training data sequences).\n",
    "\n",
    "* Can each token in the decoder be allowed to search for relevant tokens in the input sentence? This search has to be soft i.e. predict probabilities over the input sequence rather than hard choices.\n",
    "\n",
    "* In this paper, a mechanism (attention) is introduced where, at each time-step in the decoder, a soft search over all hidden states in the encoder is carried out. This search is used to generate a new appropriate context vector that focuses on subsets of the input sequence. In other words, there is no need to encode the full input sequence into *one* context vector. Instead each input sequence is encoded into a sequence of vectors and during decoding, a soft search is carried out over this sequence of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7edcce",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "While in the previous paper, the final hidden state in the decoder, $h_T$ is used as a context vector, as this paper suggests, one could generalize to:\n",
    "\n",
    "$$c = q({h_1, \\ldots, h_T})$$\n",
    "\n",
    "i.e. the context vector is some (fixed) function of all the hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc57bc",
   "metadata": {},
   "source": [
    "**Core idea figure**:\n",
    "\n",
    "![Model](seq2seq_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39db22c",
   "metadata": {},
   "source": [
    "The $x_t$ are the input tokens to the encoder. The $h_t$ are the hidden states (bidirectional). The key difference is the computation of the decoder's hidden state at time t, $s_t$.\n",
    "\n",
    "In the classic encoder-decoder picture, $s_t = f(s_{t-1}, y_{t-1})$. In this model,\n",
    "\n",
    "$$s_t = f(s_{t-1}, y_{t-1}, h_{1}, h_{2}, \\ldots, h_T)$$\n",
    "\n",
    "where $f()$ sloppily refers to \"some function\" (not the same one in both computations). So, now the computation has direct access to each encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9441aa2",
   "metadata": {},
   "source": [
    "**Precise formulation**:\n",
    "\n",
    "Given an input sequence S, of length T, the encoder produces hidden states, $h_1, \\ldots, h_T$.\n",
    "\n",
    "These hidden states are used to compute a context vector, $c$. In the seq2seq paper, $c = h_T$.\n",
    "\n",
    "The decoder conditions on the context vector i.e. it intializes its hidden state, $s_0$ so that $s_0 = c$.\n",
    "\n",
    "\n",
    "The first token is a special SOS (start of sentence) token and is passed as the first input $y_0$. The decoder hidden state is updated:\n",
    "\n",
    "$$s_1 = f(s_0, y_0)$$\n",
    "\n",
    "which is used to compute the probability distribution over the vocabulary:\n",
    "\n",
    "$$p(y_1\\mid y_0, c) = g(s_1)$$\n",
    "\n",
    "\n",
    "For any time t,\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1})}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}, c) = g(s_{t})}$$\n",
    "\n",
    "Both of these equations implictly depend on the context vector $c$ since every calculation depends on $s_0 = c$. We make this explicit:\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1}, c)}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}, c) = g(s_{t}, c)}$$\n",
    "\n",
    "One way of looking at attention is to make $c$ dependent on time t, i.e.:\n",
    "\n",
    "$$\\boxed{s_t = f(s_{t-1}, y_{t-1}, c_t)}$$\n",
    "\n",
    "and $$\\boxed{p(y_t\\mid y_0,\\ldots, y_{t-1}) = g(s_{t}, c_t)}$$\n",
    "\n",
    "\n",
    "This implies an order of computation:\n",
    "\n",
    "* Compute $c_t$ which is the context vector at time t.\n",
    "\n",
    "* Compute hidden state $s_t$ from $c_t, s_{t-1}, y_{t-1}$.\n",
    "\n",
    "* Compute prob distribution from $s_t, c_t$.\n",
    "\n",
    "The context vector, instead of being just $h_T$ (last hidden vector in the encoder) is now generalized to a linear (convex) combination of all encoder hidden vectors:\n",
    "\n",
    "$$c_t = \\Sigma_{j} \\alpha_{tj}h_j$$\n",
    "\n",
    "where $\\alpha_{tj}$ can be interpreted as probabilities as shown below.\n",
    "\n",
    "$$\\alpha_{tj} = \\frac{e^{e_{tj}}}{\\Sigma_{k}e^{e_{tk}}}$$\n",
    "\n",
    "This is essentially a Boltzmann probability or you can think of $e_{tj}$ as the energy of the $j$th configuration, the exponential ensures that $\\alpha_{tj} > 0$ and the denominator ensures the $\\alpha_{tj}$ add up to 1 (when summed over $j$). \n",
    "\n",
    "The next question is how $e_{tj}$ is computed and there are many choices here. In general, $e_{tj} = a(s_{t-1}, h_j)$. Note that $e_{tj}$ is computed before $s_t$ and hence depends on $s_{t-1}$ and not $s_t$. In words, \"based on what the decoder has generated so far till time t-1 and the summary encoded in $s_{t-1}$, what information can be gathered from the encoder's hidden states to compute the next hidden state $s_t$ so the next token can be computed\".\n",
    "\n",
    "**Open question**: what would happen if $s_t$ were used to compute $c_t$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b8d69",
   "metadata": {},
   "source": [
    "**Model architecture**:\n",
    "\n",
    "Encoder: Bidirectional RNN (LSTM etc.)\n",
    "\n",
    "Decoder: RNN (LSTM etc.) + Attention as described above\n",
    "\n",
    "Loss: multi-class log loss/cross-entropy\n",
    "\n",
    "Decoding strategy: Beam search\n",
    "\n",
    "Optimizer: Adadelta (training time for paper's model ~ 5 days on what hardware?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590279d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
