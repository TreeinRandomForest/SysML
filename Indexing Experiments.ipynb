{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "59c8ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "03076970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "367e3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4fc0323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(N=100, d=10, low=0, high=10, target_idx=3):\n",
    "    data = np.random.randint(low=low, high=high, size=(N,d))\n",
    "    return data, data[:, target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "290cdc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 4, 2, 7, 0, 2, 7, 4, 2])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "592a6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "train_data, train_target = gen_data(N=N)\n",
    "test_data, test_target = gen_data(N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22daa5b",
   "metadata": {},
   "source": [
    "Input array: a\n",
    "\n",
    "Fixed index: i\n",
    "\n",
    "Experiment 1: Always predict entry in fixed index position: a $\\rightarrow$ a[i] #current PC\n",
    "\n",
    "Experiment 2: a $\\rightarrow$ a[target_idx + a[i]] #going to correct instruction location - offset\n",
    "\n",
    "Experiment 3: conditional on a[target_idx + a[i]] #increment to next PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9076d0c",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "91be6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10 #length of generated arrays\n",
    "low = 0 #values in array, v satisfy low <= v <= high-1 \n",
    "high = 10 #values in array, v satisfy low <= v <= high-1\n",
    "n_hidden = 10 #if net is MLP, number of hidden nodes (with 1 hidden layer)\n",
    "target_idx = 3 #target is array[target_idx]\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d495c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.from_numpy(train_data[0:5]).float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cf90918f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.771106004714966 train_acc = 0.091 test_acc = 0.090 train_rmse = 4.244  test_rmse = 4.266\n",
      "Epoch = 100 loss=2.29949688911438 train_acc = 0.111 test_acc = 0.108 train_rmse = 3.624  test_rmse = 3.674\n",
      "Epoch = 200 loss=1.6697866916656494 train_acc = 0.300 test_acc = 0.309 train_rmse = 1.708  test_rmse = 1.710\n",
      "Epoch = 300 loss=1.4468904733657837 train_acc = 0.490 test_acc = 0.487 train_rmse = 1.138  test_rmse = 1.149\n",
      "Epoch = 400 loss=1.3311940431594849 train_acc = 0.590 test_acc = 0.587 train_rmse = 0.959  test_rmse = 0.963\n",
      "Epoch = 500 loss=1.2496953010559082 train_acc = 0.607 test_acc = 0.609 train_rmse = 0.884  test_rmse = 0.880\n",
      "Epoch = 600 loss=1.1855201721191406 train_acc = 0.609 test_acc = 0.608 train_rmse = 0.843  test_rmse = 0.842\n",
      "Epoch = 700 loss=1.1319814920425415 train_acc = 0.614 test_acc = 0.608 train_rmse = 0.833  test_rmse = 0.841\n",
      "Epoch = 800 loss=1.0860111713409424 train_acc = 0.619 test_acc = 0.612 train_rmse = 0.830  test_rmse = 0.839\n",
      "Epoch = 900 loss=1.0461474657058716 train_acc = 0.640 test_acc = 0.635 train_rmse = 0.817  test_rmse = 0.825\n",
      "Epoch = 1000 loss=1.0111942291259766 train_acc = 0.681 test_acc = 0.679 train_rmse = 0.791  test_rmse = 0.798\n",
      "Epoch = 1100 loss=0.9802024364471436 train_acc = 0.702 test_acc = 0.700 train_rmse = 0.778  test_rmse = 0.785\n",
      "Epoch = 1200 loss=0.9524332284927368 train_acc = 0.702 test_acc = 0.701 train_rmse = 0.778  test_rmse = 0.784\n",
      "Epoch = 1300 loss=0.927317202091217 train_acc = 0.702 test_acc = 0.701 train_rmse = 0.778  test_rmse = 0.784\n",
      "Epoch = 1400 loss=0.9044015407562256 train_acc = 0.703 test_acc = 0.702 train_rmse = 0.777  test_rmse = 0.784\n",
      "Epoch = 1500 loss=0.8833422064781189 train_acc = 0.749 test_acc = 0.740 train_rmse = 0.747  test_rmse = 0.759\n",
      "Epoch = 1600 loss=0.863875687122345 train_acc = 0.784 test_acc = 0.776 train_rmse = 0.723  test_rmse = 0.735\n",
      "Epoch = 1700 loss=0.8457245230674744 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 1800 loss=0.8290314674377441 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 1900 loss=0.8128858208656311 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2000 loss=0.79788738489151 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2100 loss=0.7837077379226685 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2200 loss=0.7702553868293762 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2300 loss=0.7574546337127686 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2400 loss=0.7451643347740173 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2500 loss=0.7335180044174194 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2600 loss=0.7223304510116577 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2700 loss=0.7114238142967224 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2800 loss=0.7010601758956909 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 2900 loss=0.6909400820732117 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3000 loss=0.6814367175102234 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3100 loss=0.671866238117218 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3200 loss=0.662808358669281 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3300 loss=0.6540120840072632 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3400 loss=0.6455038785934448 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3500 loss=0.6371839642524719 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3600 loss=0.629106342792511 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3700 loss=0.6213520169258118 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3800 loss=0.6136606335639954 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3900 loss=0.6062420010566711 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4000 loss=0.5990167260169983 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4100 loss=0.5919617414474487 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4200 loss=0.5850693583488464 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4300 loss=0.5783541798591614 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4400 loss=0.5718007683753967 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4500 loss=0.5654038786888123 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4600 loss=0.5591569542884827 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4700 loss=0.5530532598495483 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4800 loss=0.5470901131629944 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4900 loss=0.5412636399269104 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5000 loss=0.5355711579322815 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5100 loss=0.5300095677375793 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5200 loss=0.524575412273407 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5300 loss=0.5192641615867615 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5400 loss=0.5140710473060608 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5500 loss=0.5089947581291199 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5600 loss=0.5040353536605835 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5700 loss=0.4991917610168457 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5800 loss=0.4944609999656677 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5900 loss=0.4898393154144287 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6000 loss=0.4853268563747406 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6100 loss=0.4809219241142273 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6200 loss=0.47662216424942017 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6300 loss=0.47242647409439087 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6400 loss=0.46833398938179016 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6500 loss=0.4643423855304718 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6600 loss=0.46045130491256714 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6700 loss=0.4566584527492523 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6800 loss=0.4529625475406647 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6900 loss=0.44936269521713257 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7000 loss=0.44585731625556946 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7100 loss=0.442444771528244 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7200 loss=0.4391239285469055 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7300 loss=0.4358927607536316 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7400 loss=0.4327499568462372 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7500 loss=0.42969414591789246 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7600 loss=0.42672422528266907 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7700 loss=0.4238373637199402 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7800 loss=0.4210328757762909 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7900 loss=0.41830870509147644 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8000 loss=0.4156631827354431 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8100 loss=0.4130952060222626 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8200 loss=0.4106026887893677 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8300 loss=0.4081830382347107 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8400 loss=0.40583667159080505 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8500 loss=0.4035598039627075 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8600 loss=0.40134838223457336 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8700 loss=0.3992156386375427 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8800 loss=0.39714178442955017 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8900 loss=0.39512020349502563 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9000 loss=0.39316439628601074 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9100 loss=0.39128220081329346 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9200 loss=0.38944751024246216 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9300 loss=0.3876805007457733 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9400 loss=0.385944128036499 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9500 loss=0.38429775834083557 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9600 loss=0.3826862573623657 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9700 loss=0.3811078369617462 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9800 loss=0.379601389169693 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9900 loss=0.3781431317329407 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 10000\n",
    "\n",
    "def validate(net, test_data, test_target):\n",
    "    net = net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_data_torch = torch.from_numpy(test_data).float().to(device)\n",
    "        test_target_torch = torch.from_numpy(test_target).float().to(device)\n",
    "\n",
    "        test_pred = net(test_data_torch).argmax(dim=1)\n",
    "        test_accuracy = (test_target_torch == test_pred).float().mean().to('cpu').detach().numpy()\n",
    "\n",
    "        test_rmse = ((test_pred-test_target_torch)**2).mean().sqrt().to('cpu').detach().numpy()\n",
    "\n",
    "    return test_accuracy, test_rmse\n",
    "\n",
    "def train_net(net, train_data, train_target, test_data, test_target, N_epochs=10, print_freq=100):\n",
    "    net = net.to(device)\n",
    "    train_data_torch = torch.from_numpy(train_data).float().to(device)\n",
    "    train_target_torch = torch.from_numpy(train_target).to(device)\n",
    "    \n",
    "    net = net.train()\n",
    "    for i in range(N_epochs):\n",
    "        pred = net(train_data_torch)\n",
    "        loss = criterion(pred, train_target_torch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            train_accuracy, train_rmse = validate(net, train_data, train_target)\n",
    "            test_accuracy, test_rmse = validate(net, test_data, test_target)\n",
    "            net = net.train()\n",
    "            print(f'Epoch = {i} loss={loss} train_acc = {train_accuracy:.3f} test_acc = {test_accuracy:.3f} train_rmse = {train_rmse:.3f}  test_rmse = {test_rmse:.3f}')\n",
    "            \n",
    "    return net\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e365d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tensor([-0.3015,  0.1111,  0.2111, -0.3045, -0.2890,  0.0473, -0.1259, -0.2070,\n",
      "        -0.2045, -0.3144], grad_fn=<SelectBackward0>)\n",
      "tensor(6.4355, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0667, -0.1944,  0.0216,  0.1067,  0.1495, -0.0623,  0.2018, -0.1545,\n",
      "        -0.0649, -0.3151], grad_fn=<SelectBackward0>)\n",
      "tensor(4.9419, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.1088,  0.2854, -0.2113,  0.0557, -0.1331,  0.2542, -0.0837,  0.1790,\n",
      "         0.3101,  0.0939], grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.0322,  0.2941,  0.1110, -0.1791,  0.2127,  0.1184, -0.1106,  0.2899,\n",
      "         0.2439,  0.0996], grad_fn=<SelectBackward0>)\n",
      "tensor(5.5586, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.3080, -0.1598, -0.3045, -0.2603,  0.1060, -0.0187,  0.2244, -0.0984,\n",
      "         0.1602,  0.2788], grad_fn=<SelectBackward0>)\n",
      "tensor(13.8863, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0815,  0.1844, -0.1693,  0.0600, -0.2865,  0.0814, -0.0856,  0.0808,\n",
      "        -0.2522, -0.0682], grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.1739,  0.0727, -0.2421, -0.2050, -0.0975,  0.0063,  0.1216,  0.2473,\n",
      "        -0.0782,  0.2024], grad_fn=<SelectBackward0>)\n",
      "tensor(32.7561, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.1377, -0.2286, -0.1070, -0.1348,  0.1392, -0.2988, -0.1628,  0.1888,\n",
      "        -0.1279, -0.0040], grad_fn=<SelectBackward0>)\n",
      "tensor(33.4646, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.1379, -0.2634, -0.0251, -0.1933,  0.2454,  0.1605,  0.1827,  0.1709,\n",
      "        -0.2573,  0.1363], grad_fn=<SelectBackward0>)\n",
      "tensor(7.7072, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.2298,  0.2194,  0.2526,  0.2857, -0.2314,  0.0255,  0.2487, -0.3092,\n",
      "        -0.1147, -0.1696], grad_fn=<SelectBackward0>)\n",
      "tensor(11.2022, grad_fn=<DivBackward0>)\n",
      "***\n",
      "---\n",
      "tensor([ 0.2258,  0.1332,  0.1845, -0.0225,  0.0876,  0.1157,  0.1256,  0.3101,\n",
      "         0.0139,  0.2178], grad_fn=<SelectBackward0>)\n",
      "tensor(1.6165, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.2833,  0.2926,  0.0577,  0.0035,  0.0979, -0.2058, -0.2654, -0.1208,\n",
      "        -0.2551,  0.1455], grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0358, -0.2622,  0.2807,  0.1932,  0.1064, -0.2104,  0.0682, -0.0043,\n",
      "        -0.1205,  0.2960], grad_fn=<SelectBackward0>)\n",
      "tensor(44.8814, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0828, -0.1732, -0.3119, -0.2885, -0.1266,  0.2837,  0.1576,  0.2909,\n",
      "         0.1675,  0.2851], grad_fn=<SelectBackward0>)\n",
      "tensor(3.4847, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0125,  0.2397,  0.2675,  0.1460,  0.0427,  0.1122, -0.0882,  0.0599,\n",
      "        -0.0665, -0.1955], grad_fn=<SelectBackward0>)\n",
      "tensor(11.7094, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.2351, -0.3052, -0.2310, -0.2881,  0.2485, -0.2508,  0.0875, -0.1961,\n",
      "         0.2491, -0.1678], grad_fn=<SelectBackward0>)\n",
      "tensor(3.2922, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.2933, -0.1914, -0.1707, -0.2006,  0.2393,  0.1813,  0.0645,  0.1819,\n",
      "        -0.1888, -0.1735], grad_fn=<SelectBackward0>)\n",
      "tensor(3.1111, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([ 0.0458, -0.1909, -0.1748,  0.0249,  0.2560,  0.1510,  0.1812, -0.1467,\n",
      "        -0.1101, -0.2690], grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.0673,  0.2372,  0.2988,  0.0192,  0.0580, -0.2892, -0.2260,  0.0160,\n",
      "         0.3095, -0.1047], grad_fn=<SelectBackward0>)\n",
      "tensor(1.1999, grad_fn=<DivBackward0>)\n",
      "---\n",
      "tensor([-0.1253,  0.2787,  0.2673, -0.0353,  0.2553,  0.2639, -0.2399, -0.1175,\n",
      "        -0.2720,  0.2835], grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#weights are not \"peaky\"\n",
    "\n",
    "for _ in range(n_hidden):\n",
    "    l = list(net.parameters())[0][_]\n",
    "    \n",
    "    print('---')\n",
    "    print(l)\n",
    "    print(l[target_idx].abs() / l.abs().min())\n",
    "    \n",
    "print('***')\n",
    "for _ in range(high-low):\n",
    "    l = list(net.parameters())[2][_]\n",
    "    \n",
    "    print('---')\n",
    "    print(l)\n",
    "    \n",
    "    print(l[target_idx].abs() / l.abs().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "80696b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1752,  0.3209, -0.4630, -0.7688, -0.1848,  0.3138,  0.4108,  0.3876,\n",
       "        -0.3857, -0.0603], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())[0].sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f2c8e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3773, -0.2450,  0.8590,  1.0475,  0.2358, -0.3738,  0.2014, -0.5981,\n",
       "         0.0190, -0.0229], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())[0].sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2a6ba06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 17.,  9., 17.,  4., 15., 11., 17.,  7., 12.], device='cuda:0')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_torch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "761ba92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.469349  ,  0.64563804,  3.7054089 ,  4.40810317,  2.95738274,\n",
       "       -0.04907821,  2.83234672, -1.58928016,  0.44751332,  0.03399417])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(np.array(list(net.parameters())[0].cpu().detach()), test_data[2]) + np.array(list(net.parameters())[1].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fb492",
   "metadata": {},
   "source": [
    "### Experiments 1X:\n",
    "\n",
    "1a. what happens when number of hidden nodes is decreased from 20?\n",
    "\n",
    "1b. 2d (t-sne) visualizations\n",
    "\n",
    "    raw data -> t-sne (colors are the target value)\n",
    "    \n",
    "    raw data -> nn -> take the activations (20 intermediate values) -> t-sne (colors are the target value)\n",
    "    \n",
    "1c. interpreting nns: which input element has the most effect on the output\n",
    "\n",
    "1d. paths from each input node to each output node\n",
    "\n",
    "1e. compute derivatives of output nodes w.r.t. input nodes and look at max absolute value\n",
    "\n",
    "1f. replace net by convolutional net and check weights\n",
    "\n",
    "1g. try more hidden layers\n",
    "\n",
    "1h. try attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047eabaf",
   "metadata": {},
   "source": [
    "### Experiment 1a: vary n_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "cb57a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.6532723903656006 train_acc = 0.103 test_acc = 0.091 train_rmse = 3.908  test_rmse = 3.926\n",
      "Epoch = 1000 loss=1.1496717929840088 train_acc = 0.611 test_acc = 0.605 train_rmse = 0.984  test_rmse = 1.000\n",
      "Epoch = 2000 loss=0.8222751021385193 train_acc = 0.799 test_acc = 0.789 train_rmse = 0.713  test_rmse = 0.726\n",
      "Epoch = 3000 loss=0.6934895515441895 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4000 loss=0.6076760292053223 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5000 loss=0.5427573919296265 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6000 loss=0.4914427101612091 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7000 loss=0.45086073875427246 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8000 loss=0.41952523589134216 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9000 loss=0.3960285186767578 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 1\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1bb8a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.644395112991333 train_acc = 0.105 test_acc = 0.091 train_rmse = 3.576  test_rmse = 3.651\n",
      "Epoch = 1000 loss=0.990158200263977 train_acc = 0.703 test_acc = 0.701 train_rmse = 0.778  test_rmse = 0.784\n",
      "Epoch = 2000 loss=0.7899806499481201 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.721\n",
      "Epoch = 3000 loss=0.6769148707389832 train_acc = 0.803 test_acc = 0.794 train_rmse = 0.710  test_rmse = 0.721\n",
      "Epoch = 4000 loss=0.5960864424705505 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n",
      "Epoch = 5000 loss=0.5335060358047485 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n",
      "Epoch = 6000 loss=0.4838044345378876 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n",
      "Epoch = 7000 loss=0.4445449113845825 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n",
      "Epoch = 8000 loss=0.41438260674476624 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n",
      "Epoch = 9000 loss=0.3918471336364746 train_acc = 0.803 test_acc = 0.793 train_rmse = 0.710  test_rmse = 0.723\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 2\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f4572f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.5127551555633545 train_acc = 0.064 test_acc = 0.062 train_rmse = 4.560  test_rmse = 4.553\n",
      "Epoch = 1000 loss=1.1365019083023071 train_acc = 0.611 test_acc = 0.605 train_rmse = 0.834  test_rmse = 0.844\n",
      "Epoch = 2000 loss=0.7987487316131592 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 3000 loss=0.6724029779434204 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 4000 loss=0.5889480710029602 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 5000 loss=0.5264238715171814 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 6000 loss=0.4775548279285431 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 7000 loss=0.43960732221603394 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 8000 loss=0.41086357831954956 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n",
      "Epoch = 9000 loss=0.3896052837371826 train_acc = 0.801 test_acc = 0.793 train_rmse = 0.711  test_rmse = 0.723\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 3\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "21ba19b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.3994696140289307 train_acc = 0.097 test_acc = 0.103 train_rmse = 5.352  test_rmse = 5.375\n",
      "Epoch = 1000 loss=0.16526924073696136 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 2000 loss=0.03721220791339874 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 3000 loss=0.014808270148932934 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 4000 loss=0.007174258586019278 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 5000 loss=0.0038195305969566107 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 6000 loss=0.002136622089892626 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 7000 loss=0.0012317924993112683 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 8000 loss=0.0007227032911032438 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 9000 loss=0.0004288126074243337 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 4\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abae7ac",
   "metadata": {},
   "source": [
    "### Question for experiment 1a: why is n_hidden=4 where a sudden transition in performance takes place? Is it related to target_idx=3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8ca39bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "train_data, train_target = gen_data(N=N, target_idx=6)\n",
    "test_data, test_target = gen_data(N=N, target_idx=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c471a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.772714376449585 train_acc = 0.095 test_acc = 0.091 train_rmse = 4.494  test_rmse = 4.515\n",
      "Epoch = 1000 loss=2.301755905151367 train_acc = 0.106 test_acc = 0.096 train_rmse = 4.540  test_rmse = 4.522\n",
      "Epoch = 2000 loss=2.301205635070801 train_acc = 0.107 test_acc = 0.096 train_rmse = 4.536  test_rmse = 4.517\n",
      "Epoch = 3000 loss=2.2999746799468994 train_acc = 0.107 test_acc = 0.097 train_rmse = 4.520  test_rmse = 4.502\n",
      "Epoch = 4000 loss=0.8183386325836182 train_acc = 0.796 test_acc = 0.781 train_rmse = 0.452  test_rmse = 0.468\n",
      "Epoch = 5000 loss=0.6665934324264526 train_acc = 0.809 test_acc = 0.794 train_rmse = 0.437  test_rmse = 0.454\n",
      "Epoch = 6000 loss=0.5768826007843018 train_acc = 0.809 test_acc = 0.794 train_rmse = 0.437  test_rmse = 0.454\n",
      "Epoch = 7000 loss=0.5124085545539856 train_acc = 0.809 test_acc = 0.794 train_rmse = 0.437  test_rmse = 0.454\n",
      "Epoch = 8000 loss=0.46353697776794434 train_acc = 0.809 test_acc = 0.794 train_rmse = 0.437  test_rmse = 0.454\n",
      "Epoch = 9000 loss=0.4264775514602661 train_acc = 0.809 test_acc = 0.794 train_rmse = 0.437  test_rmse = 0.454\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 1\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3267b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=3.048898696899414 train_acc = 0.104 test_acc = 0.119 train_rmse = 3.477  test_rmse = 3.484\n",
      "Epoch = 1000 loss=1.1101036071777344 train_acc = 0.609 test_acc = 0.584 train_rmse = 0.837  test_rmse = 0.846\n",
      "Epoch = 2000 loss=0.4700790047645569 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 3000 loss=0.23210586607456207 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 4000 loss=0.1511794626712799 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 5000 loss=0.10203485190868378 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 6000 loss=0.06872177869081497 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 7000 loss=0.04601314291357994 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 8000 loss=0.030869195237755775 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 9000 loss=0.020733289420604706 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 2\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e9e6cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.9217448234558105 train_acc = 0.126 test_acc = 0.116 train_rmse = 4.126  test_rmse = 4.136\n",
      "Epoch = 1000 loss=0.12990738451480865 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.060  test_rmse = 0.014\n",
      "Epoch = 2000 loss=0.017265046015381813 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.014\n",
      "Epoch = 3000 loss=0.005189168732613325 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 4000 loss=0.0021064416505396366 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 5000 loss=0.0009787054732441902 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 6000 loss=0.0004875070007983595 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 7000 loss=0.00025403936160728335 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 8000 loss=0.0001347991346847266 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 9000 loss=7.23699267837219e-05 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 3\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65667f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=2.7339725494384766 train_acc = 0.119 test_acc = 0.110 train_rmse = 4.634  test_rmse = 4.647\n",
      "Epoch = 1000 loss=0.14368084073066711 train_acc = 0.999 test_acc = 1.000 train_rmse = 0.028  test_rmse = 0.000\n",
      "Epoch = 2000 loss=0.01617692969739437 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.014  test_rmse = 0.000\n",
      "Epoch = 3000 loss=0.004698417615145445 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 4000 loss=0.0018843208672478795 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 5000 loss=0.0008788132690824568 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 6000 loss=0.0004434954607859254 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 7000 loss=0.00023419401259161532 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 8000 loss=0.00012716562196146697 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 9000 loss=7.030292908893898e-05 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 4\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d1b99",
   "metadata": {},
   "source": [
    "### Note: When target_idx was changed to 6, even n_hidden=2 worked well (in terms of performance on test set). Why?\n",
    "\n",
    "Plot 1: Plot input vectors\n",
    "* Input vectors are 10d\n",
    "* Use t-sne to map it to 2d and plot. Color each point by the target value\n",
    "\n",
    "Plot 2: Plot hidden activations (n_hidden=2 or n_hidden=3)\n",
    "* Plot hidden layer activations for each input vector and color them by the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b7c87530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss=3.2268967628479004 train_acc = 0.108 test_acc = 0.103 train_rmse = 4.337  test_rmse = 4.337\n",
      "Epoch = 1000 loss=1.109921932220459 train_acc = 0.609 test_acc = 0.587 train_rmse = 0.836  test_rmse = 0.844\n",
      "Epoch = 2000 loss=0.9139060378074646 train_acc = 0.706 test_acc = 0.694 train_rmse = 0.776  test_rmse = 0.777\n",
      "Epoch = 3000 loss=0.2796461880207062 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 4000 loss=0.16456685960292816 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 5000 loss=0.11058631539344788 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 6000 loss=0.07540344446897507 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 7000 loss=0.051220472902059555 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 8000 loss=0.03460615873336792 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n",
      "Epoch = 9000 loss=0.0234210304915905 train_acc = 1.000 test_acc = 1.000 train_rmse = 0.000  test_rmse = 0.000\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 2\n",
    "\n",
    "net = nn.Sequential(nn.Linear(d, n_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_hidden, high-low))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "net = train_net(net, train_data, train_target, test_data, test_target, N_epochs=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2cc93d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 142.4816,  138.6892,  127.0850,  105.7965,   71.8712,   13.4982,\n",
       "        -754.4501, -165.0163, -167.9870, -192.0000], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.from_numpy(train_data[0]).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b4b39977",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "act = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b80e1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_act = torch.matmul(params[0], torch.from_numpy(train_data[0]).float().to(device)) + params[1]\n",
    "\n",
    "layer1_act_relu = act(layer1_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6f3c8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_act = torch.matmul(params[2], layer1_act_relu) + params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "12182f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 142.4816,  138.6892,  127.0850,  105.7965,   71.8712,   13.4982,\n",
       "        -754.4501, -165.0163, -167.9870, -192.0000], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3a691bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 18.6105, -14.9387], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "97cc5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_act = torch.matmul(torch.from_numpy(train_data).float().to(device), params[0].T) + params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "40a57d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_act = layer1_act.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "633972af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f63a3682e80>]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAI/CAYAAACvYncDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV+UlEQVR4nO3dcayd913f8c+XuJCpIxIoJpS0xmmUakoRS7tDtkoFNVKUpHhSlqJWAQ1RFWQ6tdKq8Y9ZmZp1qhRtZUyaOooHXdA0U0XLQrO6tCVACX+RXpfIOCQRceKqsULqrEKgqkob+7c/7kl641z7+vr6e59z7329pCuf5znnnuerR0+O33nOOY9rjBEAAC6975l6AACA7UpoAQA0EVoAAE2EFgBAE6EFANBEaAEANNk19QArXXnllWPv3r1TjwEAsKYjR448P8bYfb7HLFRo7d27N0tLS1OPAQCwpqr66lqP8dYhAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNdk09wGb6/JdP5v33PfLy8id/5obc9hNXTzcQALCt7ZjQ2nvg8KvWvf++R5L7HsmJu/dt/kAAwLbnrUMAgCZCCwCgidACAGgitAAAmggtAIAmQgsAoMmOubyDSzgAAJvNGS0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJru6N1BVJ5L8fZLTSV4cY8y6twkAsAjaQ2vupjHG85u0LQCAheCtQwCAJpsRWiPJF6vqSFXt34TtAQAshM146/DtY4yTVfVDSf6wqh4fYzz00p3z+NqfJHv27NmEcQAANkf7Ga0xxsn5n19Pcn+SG8+6/+AYYzbGmO3evbt7HACATdMaWlX12qr6/pduJ7klybHObQIALIrutw6vSnJ/Vb20rUNjjM83bxMAYCG0htYY46kk/7hzGwAAi8rlHQAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBosmvqAbh0/vm/P5xj3/ru8o/9g+SzH9k33UAAsMM5o7VN7D3wyshKkmPfWl4PAExDaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWtvEv/yJ169rPQDQr8YYU8/wstlsNpaWlqYeAwBgTVV1ZIwxO99jnNECAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK02DJOnxk5+NDx3PDRL+bgQ8dz+syYeiQAOK8aY3H+sprNZmNpaWnqMVhATz//zdz08S+9av1rkzx6975NnwcAqurIGGN2vsc4o8WWsFpkJck3k+w9cHhTZwGACyW0AACaCC0AgCZCCwCgidACAGgitNgSbrruB6ceAQDWTWixJfyPX3xbjt51S664fNcr1l9x+a4cveuWiaYCgPPbtfZDYDFccflrcvSuW6ceAwAumDNaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQpD20quq2qnqiqp6sqgPd2wMAWBStoVVVlyX5RJJ3Jrk+yc9W1fWd2wQAWBTdZ7RuTPLkGOOpMca3k3w6ye3N2wQAWAjdoXV1kq+tWH5mvg4AYNub/MPwVbW/qpaqaunUqVNTjwMAcMl0h9bJJG9Ysfz6+bqXjTEOjjFmY4zZ7t27m8cBANg83aH15STXVdU1VfW9Se5M8kDzNgEAFsKuzicfY7xYVR9M8oUklyX51Bjj0c5tAgAsitbQSpIxxueSfK57OwAAi2byD8MDAGxXQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCbtV4YHLtyXjj6X9x5aenn5np+b5R0/ftWEEwGwEUILFsS1Bw7n9Fnr3ntoKZcdSo7fvW+SmQDYGG8dwoI4O7LWWg/A4hNaAABNhBYAQBOhBQDQRGgBADQRWgAATVzeARbECZdwANh2nNECAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACa7Jp6AGDnuO3fHc7j3/nu8j96TfL5/7BvuoEAmjmjBWyKvQdeGVlJ8vh3ltcDbFdCCwCgidACAGgitAAAmggtAIAmQgsAoInQAjbFe97yI+taD7Ad1Bhj6hleNpvNxtLS0tRjAACsqaqOjDFm53uMM1oAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAECTttCqqruq6mRVPTL/+emubQEALKJdzc//G2OMjzdvAwBgIXnrEACgSXdofbCqjlbVp6rqB5q3BQCwUDYUWlX1YFUdW+Xn9iS/meTaJDckeTbJr5/jOfZX1VJVLZ06dWoj4wAALJQaY/RvpGpvks+OMX7sfI+bzWZjaWmpfR4AgI2qqiNjjNn5HtP5rcPXrVi8I8mxrm0BACyizm8d/sequiHJSHIiyS83bgsAYOG0hdYY4+e7nhsAYCtweQcAgCZCCwCgidACuEROnxk5+NDx3PDRL+bgQ8dz+kz/t7qBxbYpl3e4UC7vAGxVTz//zdz08S+9av3NP/o9+e1/9c7NHwhoN+nlHQB2ktUiK0ke/OqZ7D1weHOHARaG0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAugRN378vRu27JFZe/8l82u+LyXTl61y0TTQVMrfMflQbYUa64/DU5etetU48BLBBntAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACa7Jp6AAC2jge/8mx+6d6vvLz82+95a25+6+smnAgWm9AC4IJcc+Bwxlnrfuner6TuTZ6+e98kM8Gi89YhABfk7Mhaaz0gtAAA2ggtAIAmQgsAoInQAgBoIrQAAJq4vAMAF+SESzjAujmjBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0GRDoVVV766qR6vqTFXNzrrvV6vqyap6oqpu3diYAABbz64N/v6xJO9K8lsrV1bV9UnuTPLmJD+S5MGqetMY4/QGtwcAsGVs6IzWGOOxMcYTq9x1e5JPjzFeGGM8neTJJDduZFsAAFtN12e0rk7ytRXLz8zXAQDsGGu+dVhVDyb54VXu+vAY4zMbHaCq9ifZnyR79uzZ6NMBACyMNUNrjHHzRTzvySRvWLH8+vm61Z7/YJKDSTKbzcZFbAsALol/euBwnluxfFWSP79731TjsA10vXX4QJI7q+r7quqaJNclebhpWwCwYXvPiqwkeW6+Hi7WRi/vcEdVPZPkbUkOV9UXkmSM8WiSe5P8VZLPJ/mAbxwCADvNhi7vMMa4P8n957jvY0k+tpHnBwDYylwZHgCgidACAGgitAAAmggtAEjyoZveuK71cCFqjMW5dNVsNhtLS0tTjwEAsKaqOjLGmJ3vMc5oAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgCwQ3z7xTN53z0P55oDh/O+ex7Ot188M/VI257QAoAd4M/++lTe9Gt/kD9+/FRGkj9+fHn5E194bOrRtjWhBQA7wM//zsOrrv9Pf/JUrj1weJOn2TmEFgDscKenHmAbE1oAAE2EFgBAE6EFANBEaAEANNk19QAAQL8Td++beoQdyRktAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgidACAGgitAAAmggtAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaCK0AACaCC0AgCZCCwCgya6pBwAAuFS+dPS5vPfQ0svL9/zcLO/48asmm0doAQDbwrUHDuf0Wevee2gplx1Kjt+9b5KZNvTWYVW9u6oeraozVTVbsX5vVX2rqh6Z/3xy46MCAJzb2ZG11vrNsNEzWseSvCvJb61y3/Exxg0bfH4AgC1rQ6E1xngsSarq0kwDALCNdH7r8Jqq+ouq+tOq+snG7QAALKQ1z2hV1YNJfniVuz48xvjMOX7t2SR7xhj/r6r+SZLfr6o3jzH+bpXn359kf5Ls2bPnwicHAFhwa4bWGOPm9T7pGOOFJC/Mbx+pquNJ3pRkaZXHHkxyMElms9lY77YAABZVy+Udqmp3km+MMU5X1RuTXJfkqY5tAQAkyYmJLuFwPhu9vMMdVfVMkrclOVxVX5jf9VNJjlbVI0n+d5L3jzG+saFJAQC2mI1+6/D+JPevsv6+JPdt5LkBALY6/9YhAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANBFaAABNhBYAQBOhBQDQRGgBADQRWgAATYQWAEAToQUA0ERoAQA0EVoAAE2EFgBAE6EFANBEaAEANKkxxtQzvKyqTiX56gae4sokz1+icXYK+2z97LP1s8/Wzz5bH/tr/eyz9Tt7n/3oGGP3+X5hoUJro6pqaYwxm3qOrcQ+Wz/7bP3ss/Wzz9bH/lo/+2z9LmafeesQAKCJ0AIAaLLdQuvg1ANsQfbZ+tln62efrZ99tj721/rZZ+u37n22rT6jBQCwSLbbGS0AgIWxLUKrqt5dVY9W1Zmqmq1Yv7eqvlVVj8x/PjnlnIvkXPtsft+vVtWTVfVEVd061YyLrKruqqqTK46tn556pkVUVbfNj6Mnq+rA1PNsBVV1oqr+cn5cLU09zyKqqk9V1der6tiKdT9YVX9YVX89//MHppxx0Zxjn3kdO4+qekNV/UlV/dX878t/PV+/rmNtW4RWkmNJ3pXkoVXuOz7GuGH+8/5NnmuRrbrPqur6JHcmeXOS25L8t6q6bPPH2xJ+Y8Wx9bmph1k08+PmE0nemeT6JD87P75Y203z48pX71d3T5Zfn1Y6kOSPxhjXJfmj+TLfdU9evc8Sr2Pn82KSXxljXJ/knyX5wPw1bF3H2rYIrTHGY2OMJ6aeYys5zz67PcmnxxgvjDGeTvJkkhs3dzq2iRuTPDnGeGqM8e0kn87y8QUbMsZ4KMk3zlp9e5Lfnd/+3ST/YjNnWnTn2Gecxxjj2THGV+a3/z7JY0muzjqPtW0RWmu4pqr+oqr+tKp+cuphtoCrk3xtxfIz83W82ger6uj8lLy3KV7NsXRxRpIvVtWRqto/9TBbyFVjjGfnt/8myVVTDrOFeB27AFW1N8lbkvx51nmsbZnQqqoHq+rYKj/n+z/kZ5PsGWO8Jcm/SXKoqq7YnImnd5H7jLk19t9vJrk2yQ1ZPs5+fcpZ2VbePsZ4a5bfcv1AVf3U1ANtNWP56/S+Ur82r2MXoKr+YZL7knxojPF3K++7kGNtV+Nsl9QY4+aL+J0Xkrwwv32kqo4neVOSHfEB04vZZ0lOJnnDiuXXz9ftOBe6/6rqvyf5bPM4W5Fj6SKMMU7O//x6Vd2f5bdgV/v8Ka/0XFW9bozxbFW9LsnXpx5o0Y0xnnvpttex1VXVa7IcWf9rjPF/5qvXdaxtmTNaF6Oqdr/0Qe6qemOS65I8Ne1UC++BJHdW1fdV1TVZ3mcPTzzTwpn/x/WSO7L85QJe6ctJrquqa6rqe7P8JYsHJp5poVXVa6vq+1+6neSWOLYu1ANJfmF++xeSfGbCWbYEr2PnV1WV5HeSPDbG+M8r7lrXsbYtLlhaVXck+a9Jdif52ySPjDFuraqfSfLRJN9JcibJR8YY/3eyQRfIufbZ/L4PJ3lflr9x8aExxh9MNeeiqqr/meXT7SPJiSS/vOI9e+bmXxf/L0kuS/KpMcbHpp1osc3/h/D++eKuJIfss1erqt9L8o4kVyZ5LslHkvx+knuT7Eny1STvGWP48PfcOfbZO+J17Jyq6u1J/izJX2a5IZLk32b5c1oXfKxti9ACAFhE2/qtQwCAKQktAIAmQgsAoInQAgBoIrQAAJoILQCAJkILAKCJ0AIAaPL/AXy46XboNZupAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(layer1_act[:,0], layer1_act[:,1], 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b71db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
